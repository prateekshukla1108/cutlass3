
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Blackwell SM100 GEMMs &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'blackwell_functionality';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Blackwell Cluster Launch Control" href="blackwell_cluster_launch_control.html" />
    <link rel="prev" title="CUTLASS Grouped Kernel Schedulers" href="grouped_scheduler.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="overview.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>



<li class="toctree-l1"><a class="reference internal" href="terminology.html">CUTLASS Terminology</a></li>

<li class="toctree-l1"><a class="reference internal" href="ide_setup.html">IDE Setup for CUTLASS Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="functionality.html">Functionality</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="fundamental_types.html">Fundamental Types</a></li>

<li class="toctree-l1"><a class="reference internal" href="layout.html">Layouts and Tensors</a></li>



<li class="toctree-l1"><a class="reference internal" href="tile_iterator_concept.html">Tile Iterator Concepts</a></li>

<li class="toctree-l1"><a class="reference internal" href="pipeline.html">Synchronization primitives</a></li>

<li class="toctree-l1"><a class="reference internal" href="code_organization.html">CUTLASS Code Organization</a></li>

<li class="toctree-l1"><a class="reference internal" href="programming_guidelines.html">Programming Guidelines</a></li>

<li class="toctree-l1"><a class="reference internal" href="utilities.html">CUTLASS Utilities</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Key Operations &amp; APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gemm_api.html">CUTLASS GEMM API</a></li>



<li class="toctree-l1"><a class="reference internal" href="gemm_api_3x.html">CUTLASS 3.0 GEMM API</a></li>



<li class="toctree-l1"><a class="reference internal" href="efficient_gemm.html">Efficient GEMM in CUDA</a></li>


<li class="toctree-l1"><a class="reference internal" href="implicit_gemm_convolution.html">CUTLASS Convolution</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CUTLASS 3.x Specifics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cutlass_3x_design.html">CUTLASS 3.0 Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="cutlass_3x_backwards_compatibility.html">CUTLASS 3.0 GEMM Backwards Compatibility</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CUTE - Compositional Universal Tile Engine</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="cute/00_quickstart.html">Getting Started With CuTe</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/01_layout.html">CuTe Layouts</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/02_layout_algebra.html">CuTe Layout Algebra</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/03_tensor.html">CuTe Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/04_algorithms.html">CuTe Tensor algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/0t_mma_atom.html">CuTe’s support for Matrix Multiply-Accumulate instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/0x_gemm_tutorial.html">CuTe dense matrix-matrix multiply tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/0y_predication.html">Predication: What to do when tiling isn’t perfect</a></li>
<li class="toctree-l1"><a class="reference internal" href="cute/0z_tma_tensors.html">CuTe TMA Tensors</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features &amp; Platform Specifics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dependent_kernel_launch.html">Dependent kernel launches</a></li>
<li class="toctree-l1"><a class="reference internal" href="grouped_scheduler.html">CUTLASS Grouped Kernel Schedulers</a></li>





<li class="toctree-l1 current active"><a class="current reference internal" href="#">Blackwell SM100 GEMMs</a></li>


<li class="toctree-l1"><a class="reference internal" href="blackwell_cluster_launch_control.html">Blackwell Cluster Launch Control</a></li>

<li class="toctree-l1"><a class="reference internal" href="profiler.html">CUTLASS Profiler</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Building CUTLASS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="build/building_in_windows_with_visual_studio.html">Building on Windows with Visual Studio</a></li>





<li class="toctree-l1"><a class="reference internal" href="build/building_with_clang_as_host_compiler.html">Building with Clang as host compiler</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fblackwell_functionality.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/blackwell_functionality.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Blackwell SM100 GEMMs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Blackwell SM100 GEMMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-in-blackwell-sm100">New in Blackwell SM100</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#block-scaled-gemms">Block Scaled GEMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#blackwell-narrow-precision-data-types">Blackwell Narrow Precision Data Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layouts-tensor-alignment-requirements-to-target-tcgen05-mma-instructions">Layouts, Tensor Alignment Requirements to Target <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mma-tile-shapes-supported">MMA tile shapes supported</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epilogue-config-supported">Epilogue config supported</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-kernel-dispatch-policies">Auto Kernel Dispatch Policies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-block-scaled-kernel">Building a Block Scaled Kernel <a id="detailed_blockscale_example" name="detailed_blockscale_example"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scale-factor-layouts">Scale Factor Layouts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#blackwell-sm120-gemms">Blackwell SM120 GEMMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cluster-size">Cluster Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-layout">Tensor Layout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pingpong-v-s-cooperative-kernel-schedule">Pingpong v.s. cooperative kernel schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epilogue-schedule">Epilogue schedule:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tile-size">Tile size:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#copyright">Copyright</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="blackwell-sm100-gemms">
<h1>Blackwell SM100 GEMMs<a class="headerlink" href="#blackwell-sm100-gemms" title="Link to this heading">#</a></h1>
<p><a class="reference internal" href="#detailed_blockscale_example"><span class="xref myst"><strong>TLDR; jump to block scaled GEMM example</strong></span></a></p>
<p>Blackwell SM100 introduces <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions. <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions support all legacy types (<code class="docutils literal notranslate"><span class="pre">tfloat32_t</span></code>, <code class="docutils literal notranslate"><span class="pre">half_t</span></code>, <code class="docutils literal notranslate"><span class="pre">bfloat16_t</span></code>, <code class="docutils literal notranslate"><span class="pre">int8_t</span></code>, <code class="docutils literal notranslate"><span class="pre">uint8_t</span></code>) and
the new 4, 6, and 8-bits floating point datatypes with and without scale factors.
This document explains the new <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions supported by CUTLASS and how one can leverage CUTLASS to create
efficient SM100 GEMM kernels targeting these new mma instructions.</p>
<p>Blackwell SM100 has 7 new <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions. These instructions are 2x to 4x faster then Hopper Architecture’s WGMMA instructions.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Ptx Instruction</p></th>
<th class="head"><p>Throughput</p></th>
<th class="head"><p>Notes</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::tf32</p></td>
<td><p>2x Hopper Tf32 Tensor Core</p></td>
<td><p>MMA with A={tf32} x B={tf32} TN, NT, TT, NN layouts</p></td>
</tr>
<tr class="row-odd"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::f16</p></td>
<td><p>2x Hopper Fp16 Tensor Core</p></td>
<td><p>MMA with A={f16} x B={f16} or A={bf16} x B={bf16}  TN, NT, TT, NN layouts</p></td>
</tr>
<tr class="row-even"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::i8</p></td>
<td><p>2x Hopper I8 Tensor Core</p></td>
<td><p>MMA with A={i8} x B={i8} or A={u8} x B={u8}  TN, NT, TT, NN layouts</p></td>
</tr>
<tr class="row-odd"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::f8f6f4</p></td>
<td><p>2x Hopper Fp8 Tensor Core</p></td>
<td><p>Mixed precision MMA with A={f4,f6,f8} x B={f4,f6,f8} TN, NT, TT, NN layouts</p></td>
</tr>
<tr class="row-even"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::mxf8f6f4.block_scale</p></td>
<td><p>2x Hopper Fp8 Tensor Core</p></td>
<td><p>Block scaled mixed precision MMA with A={mxf4,mxf6,mxf8} x B={mxf4,mxf6,mxf8} with TN, NT, TT, NN layouts</p></td>
</tr>
<tr class="row-odd"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::mxf4.block_scale</p></td>
<td><p>4x Hopper Fp8 Tensor Core</p></td>
<td><p>Block scaled MMA with A={mxf4} x B={mxf4} with TN layouts</p></td>
</tr>
<tr class="row-even"><td><p>tcgen05.mma(.sp).cta_group::[1|2].kind::mxf4nvf4.block_scale.scale_vec_size::[2X|4X]</p></td>
<td><p>4x Hopper Fp8 Tensor Core</p></td>
<td><p>Block scaled MMA with A={mxf4} x B={mxf4} or A={nvf4} x B={nvf4} with TN layouts</p></td>
</tr>
</tbody>
</table>
</div>
<p>For more detailed information see <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tensorcore-5th-generation-family-instructions"><code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> PTX documentation</a>.</p>
<section id="new-in-blackwell-sm100">
<h2>New in Blackwell SM100<a class="headerlink" href="#new-in-blackwell-sm100" title="Link to this heading">#</a></h2>
<section id="block-scaled-gemms">
<h3>Block Scaled GEMMs<a class="headerlink" href="#block-scaled-gemms" title="Link to this heading">#</a></h3>
<p>Instructions with <code class="docutils literal notranslate"><span class="pre">kind</span></code> modifiers <code class="docutils literal notranslate"><span class="pre">mxf8f6f4</span></code>, <code class="docutils literal notranslate"><span class="pre">mxf4</span></code>, and <code class="docutils literal notranslate"><span class="pre">nvf4mxf4</span></code> perform matrix multiplication operations with scale
factors of the form <span class="math notranslate nohighlight">\(D = C +( A \times SFA) * (B \times SFB)\)</span>. Scale factors are applied to GEMM-K dimension such that
every 16 or 32 elements of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> matrices in K dimension have an associated scale factor (32 or 64 elements for sparse as sparse gemm compress 2x along k-dim). For example, an <span class="math notranslate nohighlight">\(M\times K\)</span>,
<span class="math notranslate nohighlight">\(A\)</span> matrix has an associated <span class="math notranslate nohighlight">\(M \times \lceil K/32 \rceil\)</span> SFA matrix; and an <span class="math notranslate nohighlight">\(N\times K\)</span> <span class="math notranslate nohighlight">\(B\)</span>, matrix has an associated
<span class="math notranslate nohighlight">\(N \times \lceil K/32 \rceil\)</span> SFB matrix. For block scaled GEMMs, an entry of output D matrix is
<span class="math notranslate nohighlight">\(D_{ij} = C_{ij} + \sum_{k} (A_{i,k} \times SFA_{i,k/SV}) \times (B_{j,k}\times SFB_{j,k/SV})\)</span>, in index notation, we SV is the scale factor vector size (16 or 32).
Further details can be found in
<a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#tcgen05-block-scaling">PTX documentation on block scaling</a>.</p>
</section>
<section id="blackwell-narrow-precision-data-types">
<h3>Blackwell Narrow Precision Data Types<a class="headerlink" href="#blackwell-narrow-precision-data-types" title="Link to this heading">#</a></h3>
<p>Narrow-precision <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions can operate on several 4, 6, and 8-bit data types. Blackwell MMAs can operate
on five different 8-bit floating point values, of which only two (<code class="docutils literal notranslate"><span class="pre">float_ue8m0_t</span></code> and <code class="docutils literal notranslate"><span class="pre">float_ue4m3_t</span></code>) can be used as scale factor data types.
There are two 6-bit floating point types and one 4-bit floating point data type.
See <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#alternate-floating-point-data-formats">PTX documentation for narrow precision data types</a> for details.</p>
<p><strong>Blackwell Narrow Precision Data Types</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data Type</p></th>
<th class="head"><p>Exponent Bits</p></th>
<th class="head"><p>Mantissa Bits</p></th>
<th class="head"><p>Signed</p></th>
<th class="head"><p>Bit Size</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>float_e4m3_t</p></td>
<td><p>4</p></td>
<td><p>3</p></td>
<td><p>Yes</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-odd"><td><p>float_e5m2_t</p></td>
<td><p>5</p></td>
<td><p>2</p></td>
<td><p>Yes</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-even"><td><p>float_e2m3_t</p></td>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>Yes</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-odd"><td><p>float_e3m2_t</p></td>
<td><p>3</p></td>
<td><p>2</p></td>
<td><p>Yes</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-even"><td><p>float_e2m1_t</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>Yes</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>float_ue8m0_t<a class="footnote-reference brackets" href="#id3" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>8</p></td>
<td><p>0</p></td>
<td><p>No</p></td>
<td><p>8</p></td>
</tr>
<tr class="row-even"><td><p>float_ue4m3_t<a class="footnote-reference brackets" href="#id3" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>4</p></td>
<td><p>3</p></td>
<td><p>No</p></td>
<td><p>8</p></td>
</tr>
</tbody>
</table>
</div>
<p>Block scaled MMAs use <code class="docutils literal notranslate"><span class="pre">mx</span></code> and <code class="docutils literal notranslate"><span class="pre">nv</span></code> types which are a pair of float8_t, float6_t, float4_t with 2 of the scale factor data types with a predetermined scale factor vector size. <code class="docutils literal notranslate"><span class="pre">mx</span></code> types follow OCP specification (see <a class="reference external" href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf">OCP Specification</a>). The following types provided by CUTLASS can be used as inputs to collective builders to generate the block scaled kernels:</p>
<p><strong>Blackwell Block Scaled Narrow Precision Data Types</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mx/Nv Data Type</p></th>
<th class="head"><p>Scale Factor Type</p></th>
<th class="head"><p>SF Vector Size (Dense)</p></th>
<th class="head"><p>SF Vector Size (Sparse)</p></th>
<th class="head"><p>OCP Compliant</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mx_float8_t&lt;Any F8type&gt;</p></td>
<td><p>float_ue8m0_t</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>mx_float6_t&lt;Any F6Type&gt;</p></td>
<td><p>float_ue8m0_t</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p>mx_float4_t</p></td>
<td><p>float_ue8m0_t</p></td>
<td><p>32</p></td>
<td><p>64</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>nv_float4_t</p></td>
<td><p>float_ue4m3_t</p></td>
<td><p>16</p></td>
<td><p>32</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="layouts-tensor-alignment-requirements-to-target-tcgen05-mma-instructions">
<h2>Layouts, Tensor Alignment Requirements to Target <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> Instructions<a class="headerlink" href="#layouts-tensor-alignment-requirements-to-target-tcgen05-mma-instructions" title="Link to this heading">#</a></h2>
<p>Tables below list valid data type, and AB layout combinations. Note that the alignment is reported as number of elements. A and B matrix layouts are
represented with T and N. T represents row-major layouts, and N represents column-major layouts. For instance, TN is
row-major A matrix with column-major B matrix.</p>
<p>For legacy types (<code class="docutils literal notranslate"><span class="pre">tf32</span></code>, <code class="docutils literal notranslate"><span class="pre">f16</span></code>, <code class="docutils literal notranslate"><span class="pre">bf16</span></code>, <code class="docutils literal notranslate"><span class="pre">i8</span></code> and <code class="docutils literal notranslate"><span class="pre">u8</span></code>) alignment requirements for A and B matrices are the same as in Hopper.
All four layouts (TT, NN, NT, TT) are supported for all legacy data types.</p>
<p><strong>Table 1: Valid Data Type, Alignment, and Layout Combinations For MMAs with Legacy Types</strong> <a id="legacy_gemm_table" name="legacy_gemm_table"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>A Type</p></th>
<th class="head"><p>B Type</p></th>
<th class="head"><p>AB Layout</p></th>
<th class="head"><p>A Alignment</p></th>
<th class="head"><p>B Alignment</p></th>
<th class="head"><p>Target tcgen05.mma.kind</p></th>
<th class="head"><p>Unit Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">1</span></a></p></td>
<td><p>Dense</p></td>
<td><p>tfloat32_t</p></td>
<td><p>tfloat32_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>4</p></td>
<td><p>4</p></td>
<td><p>tf32</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">2</span></a></p></td>
<td><p>Dense</p></td>
<td><p>half_t</p></td>
<td><p>half_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
<td><p>f16</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/f16_f16_void_f32.cu">Unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">3</span></a></p></td>
<td><p>Dense</p></td>
<td><p>bfloat16_t</p></td>
<td><p>bfloat16_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>8</p></td>
<td><p>8</p></td>
<td><p>f16</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/f16_f16_void_f32.cu">Similar to half_t unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">4</span></a></p></td>
<td><p>Dense</p></td>
<td><p>int8_t</p></td>
<td><p>int8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>i8</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/s8_s8_void_s32.cu">Unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">5</span></a></p></td>
<td><p>Dense</p></td>
<td><p>uint8_t</p></td>
<td><p>uint8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>i8</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/s8_s8_void_s32.cu">Similar to int8_t unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">6</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>tfloat32_t</p></td>
<td><p>tfloat32_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>4  (N) / 8 (T)</p></td>
<td><p>4</p></td>
<td><p>tf32</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_f32_f32_f32_f32_f32_tfmma.cu">Unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">7</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>half_t</p></td>
<td><p>half_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>8  (N) / 16 (T)</p></td>
<td><p>8</p></td>
<td><p>f16</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_f16_f16_f32_f16_f16_hmma.cu">Unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">8</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>bfloat16_t</p></td>
<td><p>bfloat16_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>8  (N) / 16 (T)</p></td>
<td><p>8</p></td>
<td><p>f16</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_f16_f16_f32_f16_f16_hmma.cu">Similar to half_t unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">9</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>int8_t</p></td>
<td><p>int8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16 (N) / 32 (T)</p></td>
<td><p>16</p></td>
<td><p>i8</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_s8_s8_s32_s8_s8_imma.cu">Unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#legacy_rows"><span class="xref myst">10</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>uint8_t</p></td>
<td><p>uint8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16 (N) / 32 (T)</p></td>
<td><p>16</p></td>
<td><p>i8</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_s8_s8_s32_s8_s8_imma.cu">Similar to int8_t unit tests</a></p></td>
</tr>
</tbody>
</table>
</div>
<p>For narrow precision Mmas, not all A/B type, and A/B layout combinations are supported by every <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions.
Furthermore, tensor copy instructions for subbyte types impose additional alignment requirements while loading narrow-precision
tensors from global memory to shared memory
(see <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-tensor-copy-restrictions">PTX doc</a> for details).</p>
<p>Below tables list valid layout, and alignment values for each A and B data type combination and their target <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code>
instructions supported by CUTLASS.</p>
<p><strong>Table 2: Valid Data Type, Alignment, and Layout Combinations For Narrow Precision MMAs Without Block Scaling</strong> <a id="non_bs_gemm_table" name="non_bs_gemm_table"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>A Type</p></th>
<th class="head"><p>B Type</p></th>
<th class="head"><p>AB Layout</p></th>
<th class="head"><p>A Alignment</p></th>
<th class="head"><p>B Alignment</p></th>
<th class="head"><p>Target tcgen05.mma.kind</p></th>
<th class="head"><p>Unit Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">1</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float4_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nt_layout.cu">NT unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nn_layout.cu">NN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tt_layout.cu">TT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">2</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float4_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nt_layout.cu">NT unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nn_layout.cu">NN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tt_layout.cu">TT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">3</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float6_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nt_layout.cu">NT unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nn_layout.cu">NN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tt_layout.cu">TT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_4_7"><span class="xref myst">4</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float4_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f8_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f8_void_f32_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_5_8"><span class="xref myst">5</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float8_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f8_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f8_f6f4_void_f32_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">6</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float6_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nt_layout.cu">NT unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_nn_layout.cu">NN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f6f4_void_f32_tt_layout.cu">TT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_4_7"><span class="xref myst">7</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float6_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f8_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f6f4_f8_void_f32_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_5_8"><span class="xref myst">8</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float8_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f8_f6f4_void_f32_tn_layout.cu">TN unit tests</a> <br> <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/narrow_precision/f8_f6f4_void_f32_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_9"><span class="xref myst">9</span></a></p></td>
<td><p>Dense</p></td>
<td><p>float8_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/f8_f8_void_f32.cu">Unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">10</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float4_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f4_f4_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">11</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float4_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f4_f6_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">12</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float6_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f6_f4_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_4_7"><span class="xref myst">13</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float4_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f4_f8_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_5_8"><span class="xref myst">14</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float8_t</p></td>
<td><p>float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f8_f4_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_1_2_3_6"><span class="xref myst">15</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float6_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f6_f6_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_4_7"><span class="xref myst">16</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float6_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f6_f8_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#nonbs_rows_5_8"><span class="xref myst">17</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float8_t</p></td>
<td><p>float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>128</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/narrow_precision/sm100_sp_gemm_f8_f6_f32_f16_f16_tn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#nonbs_rows_9"><span class="xref myst">18</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>float8_t</p></td>
<td><p>float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>16</p></td>
<td><p>f8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_sparse_tensorop_gemm/sm100_sp_gemm_f8_f8_f32_f16_f16_qmma.cu">TN unit tests</a></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 3: Valid Data Type, Alignment, and Layout Combinations for Block Scaled Narrow Precision MMAs</strong> <a id="bs_gemm_table" name="bs_gemm_table"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>A Type</p></th>
<th class="head"><p>B Type</p></th>
<th class="head"><p>AB Layout</p></th>
<th class="head"><p>A Alignment</p></th>
<th class="head"><p>B Alignment</p></th>
<th class="head"><p>Target tcgen05.mma.kind</p></th>
<th class="head"><p>Unit Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_1"><span class="xref myst">1</span></a></p></td>
<td><p>Dense</p></td>
<td><p>nv_float4_t</p></td>
<td><p>nv_float4_t</p></td>
<td><p>TN</p></td>
<td><p>32</p></td>
<td><p>32</p></td>
<td><p>mxf4nvf4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/nvf4_nvf4_bf16_bf16.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_2"><span class="xref myst">2</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN</p></td>
<td><p>32</p></td>
<td><p>32</p></td>
<td><p>mxf4, mxf4nvf4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf4_void_f16_tn_layout.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_3"><span class="xref myst">3</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf4_void_f16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">4</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf6_f32_f16_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf6_f32_f16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">5</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf4_f16_f16_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf4_f16_f16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">6</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf8_bf16_bf16_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf4_mxf8_bf16_bf16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">7</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf4_f16_bf16_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf4_f16_bf16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">8</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf6_void_bf16_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf6_void_bf16_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">9</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf8_void_f32_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf6_mxf8_void_f32_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">10</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf6_f16_f8_tn_layout.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf6_f16_f8_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">11</span></a></p></td>
<td><p>Dense</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf8_void_f8_tn_layout.cu.cu">TN unit tests</a><br><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_tensorop_gemm/mxf8_mxf8_void_f8_nt_layout.cu">NT unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_1"><span class="xref myst">12</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>nv_float4_t</p></td>
<td><p>nv_float4_t</p></td>
<td><p>TN</p></td>
<td><p>32  (N) / 64  (T)</p></td>
<td><p>32</p></td>
<td><p>mxf4nvf4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_nvf4_nvf4_f32_void_f16_o_tnn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_2"><span class="xref myst">13</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN</p></td>
<td><p>32  (N) / 64  (T)</p></td>
<td><p>32</p></td>
<td><p>mxf4, mxf4nvf4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf4_mxf4_f32_f16_f16_o_tnn.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_3"><span class="xref myst">14</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf4_mxf4_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">15</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf4_mxf6_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">16</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf6_mxf4_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">17</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float4_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf4_mxf8_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">18</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float4_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf8_mxf4_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">19</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf6_mxf6_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">20</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float6_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>128 (N) / 256 (T)</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf6_mxf8_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#bs_rows_4_5_7_8_10"><span class="xref myst">21</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float6_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>128</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf8_mxf6_f32_f16_f16_q_tnt.cu">TN unit tests</a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#bs_rows_6_9_11"><span class="xref myst">22</span></a></p></td>
<td><p>Sparse</p></td>
<td><p>mx_float8_t</p></td>
<td><p>mx_float8_t</p></td>
<td><p>TN, NN, NT, TT</p></td>
<td><p>16  (N) / 32  (T)</p></td>
<td><p>16</p></td>
<td><p>mxf8f6f4</p></td>
<td><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_blockscaled_sparse_tensorop_gemm/sm100_bssp_gemm_mxf8_mxf8_f32_f16_f16_q_tnn.cu">TN unit tests</a></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="mma-tile-shapes-supported">
<h2>MMA tile shapes supported<a class="headerlink" href="#mma-tile-shapes-supported" title="Link to this heading">#</a></h2>
<p>The alignment restrictions also limit the options for Mma Tile Shapes. Tables below list the supported/valid <code class="docutils literal notranslate"><span class="pre">MmaTileShape</span></code>,
Layout, and Dispatch Policy combinations for each row of <a class="reference internal" href="#legacy_gemm_table"><span class="xref myst">Table 1</span></a>, <a class="reference internal" href="#non_bs_gemm_table"><span class="xref myst">Table 2</span></a>, and <a class="reference internal" href="#bs_gemm_table"><span class="xref myst">Table 3</span></a>.</p>
<p><strong>Table 4: Valid Tile Shapes and Dispatch Policies for legacy types (All rows of Table 1)</strong> <a id="legacy_rows" name="legacy_rows"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x64x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x128x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x192x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x256x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x64x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x64x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x128x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x192x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x256x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x64x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x(4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x64x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x192x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x64x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x192x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x(2/4*MMA-K)</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 5: Valid Tile Shapes and Dispatch Policies for {float4_t, float6_t} x {float4_t, float6_t} (Rows 1,2,3,6,10,11,12,and 15 of Table 2)</strong> <a id="nonbs_rows_1_2_3_6" name="nonbs_rows_1_2_3_6"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 6: Valid Tile Shapes and Dispatch Policies for float8_t x {float4_t, float6_t} (Rows 5,8,14,and 17 of Table 2)</strong> <a id="nonbs_rows_5_8" name="nonbs_rows_5_8"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 7: Valid Tile Shapes and Dispatch Policies for {float4_t, float6_t} x float8_t (Rows 4,7,13,and 16 of Table 2)</strong> <a id="nonbs_rows_4_7" name="nonbs_rows_4_7"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 8: Valid Tile Shapes and Dispatch Policies for float8_t x float8_t (Row 9,18 of Table 2)</strong> <a id="nonbs_rows_9" name="nonbs_rows_9"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>64x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x64x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmSm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 9: Valid Tile Shapes for nv_float4_t x nv_float4_t (Row 1 and 12 of Table 3)</strong> <a id="bs_rows_1" name="bs_rows_1"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 10: Valid Tile Shapes and Dispatch Policies for mx_float4_t x mx_float4_t (Row 2 and 13 of Table 3)</strong> <a id="bs_rows_2" name="bs_rows_2"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmNvf4Sm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 11: Valid Tile Shapes and Dispatch Policies for mx_float4_t x mx_float4_t (Row 3 and 14 of Table 3)</strong> <a id="bs_rows_3" name="bs_rows_3"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x192x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 12: Valid Tile Shapes and Dispatch Policies for {mx_float4_t, mx_float6_t, mx_float8_t} x {mx_float4_t, mx_float6_t} (Rows 4, 5, 7, 8, 10, 15, 16, 18, 19, and 21 of Table 3)</strong> <a id="bs_rows_4_5_7_8_10" name="bs_rows_4_5_7_8_10"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x192x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x192x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 13: Valid Tile Shapes and Dispatch Policies for {mx_float4_t, mx_float6_t, mx_float8_t} x mx_float8_t (Rows 6, 9, 11, 17, 20, and 22 of Table 3)</strong> <a id="bs_rows_6_9_11" name="bs_rows_6_9_11"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>1SM</p></td>
<td><p>128x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x192x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>2SM</p></td>
<td><p>256x256x128</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x192x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>1SM</p></td>
<td><p>128x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x128x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x192x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>2SM</p></td>
<td><p>256x256x256</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="epilogue-config-supported">
<h2>Epilogue config supported<a class="headerlink" href="#epilogue-config-supported" title="Link to this heading">#</a></h2>
<p><strong>Table 14: Epilogue Dispatch Policy</strong> <a id="epi_dispatch" name="epi_dispatch"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dense / Sparse</p></th>
<th class="head"><p>Legacy / Narrow Precision</p></th>
<th class="head"><p>1/2 SM</p></th>
<th class="head"><p>Epilogue Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>Legacy &amp; Narrow Precision</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized1Sm</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>Legacy &amp; Narrow Precision</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::NoSmemWarpSpecialized1Sm</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Dense</p></td>
<td><p>Legacy &amp; Narrow Precision</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized2Sm</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Dense</p></td>
<td><p>Legacy &amp; Narrow Precision</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::NoSmemWarpSpecialized2Sm</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>Legacy</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized1Sm</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>Legacy</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::NoSmemWarpSpecialized1Sm</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>Legacy</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized2Sm</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>Legacy</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::NoSmemWarpSpecialized2Sm</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>Narrow Precision (nvf4)</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized1SmNvf4</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>Narrow Precision (nvf4)</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized2SmNvf4</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>Narrow Precision (mxf4)</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized1SmMxf4</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>Narrow Precision (mxf4)</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized2SmMxf4</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Sparse</p></td>
<td><p>Narrow Precision (mxf8f6f4)</p></td>
<td><p>1SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized1SmMxf8f6f4</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>Sparse</p></td>
<td><p>Narrow Precision (mxf8f6f4)</p></td>
<td><p>2SM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::TmaWarpSpecialized2SmMxf8f6f4</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 15: Epilogue PerSmTileShape_MNK</strong> <a id="epi_persmtileshape" name="epi_persmtileshape"></a></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>1/2 SM</p></th>
<th class="head"><p>MMA tile Shape</p></th>
<th class="head"><p>PerSmTileShape_MNK</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1SM</p></td>
<td><p>64x64xMMA_TileShape_K</p></td>
<td><p>64x64xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>1SM</p></td>
<td><p>64x128xMMA_TileShape_K</p></td>
<td><p>64x128xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>1SM</p></td>
<td><p>64x192xMMA_TileShape_K</p></td>
<td><p>64x192xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>1SM</p></td>
<td><p>64x256xMMA_TileShape_K</p></td>
<td><p>64x256xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>1SM</p></td>
<td><p>128x64xMMA_TileShape_K</p></td>
<td><p>128x64xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>1SM</p></td>
<td><p>128x128xMMA_TileShape_K</p></td>
<td><p>128x128xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>1SM</p></td>
<td><p>128x192xMMA_TileShape_K</p></td>
<td><p>128x192xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>1SM</p></td>
<td><p>128x256xMMA_TileShape_K</p></td>
<td><p>128x256xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>2SM</p></td>
<td><p>128x64xMMA_TileShape_K</p></td>
<td><p>64x64xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>2SM</p></td>
<td><p>128x128xMMA_TileShape_K</p></td>
<td><p>64x128xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>2SM</p></td>
<td><p>128x192xMMA_TileShape_K</p></td>
<td><p>64x192xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>2SM</p></td>
<td><p>128x256xMMA_TileShape_K</p></td>
<td><p>64x256xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>2SM</p></td>
<td><p>256x64xMMA_TileShape_K</p></td>
<td><p>128x64xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>2SM</p></td>
<td><p>256x128xMMA_TileShape_K</p></td>
<td><p>128x128xMMA_TileShape_K</p></td>
</tr>
<tr class="row-even"><td><p>2SM</p></td>
<td><p>256x192xMMA_TileShape_K</p></td>
<td><p>128x192xMMA_TileShape_K</p></td>
</tr>
<tr class="row-odd"><td><p>2SM</p></td>
<td><p>256x256xMMA_TileShape_K</p></td>
<td><p>128x256xMMA_TileShape_K</p></td>
</tr>
</tbody>
</table>
</div>
<p>MMA_TileShape_K is is generally 4 * MMA-Instruction-K. It depends on the config we defined in MMA tile shapes supported section.</p>
<section id="auto-kernel-dispatch-policies">
<h3>Auto Kernel Dispatch Policies<a class="headerlink" href="#auto-kernel-dispatch-policies" title="Link to this heading">#</a></h3>
<p>In addition to direct dispatch policies listed above, the user can also use auto policies for both non-block scaled narrow-precision
GEMMs (both sparse and dense), and block scaled narrow-precision GEMMs (only dense).</p>
<p>CUTLASS will do its best to find the most efficient kernel for given parameters, however, the preferred method for building
these kernels is to use direct kernel dispatch policies shown in the above tables.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cutlass::gemm::collective::KernelScheduleAuto</span></code>: For a given Mma Tile Size, data type and layout combinations choose instr kind (mxf8f6f4, mxf4, nvf4mxf4) and 1/2 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma(.sp)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized1SmBlockScaledSm100</span></code>: Use 1 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instruction and choose instr kind (mxf8f6f4, mxf4, nvf4mxf4) automatically.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmBlockScaledSm100</span></code>: Use 2 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instruction and choose instr kind (mxf8f6f4, mxf4, nvf4mxf4) automatically.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized1SmBlockScaledSm100</span></code>: Use 1 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma.sp</span></code> instruction and choose instr kind (mxf8f6f4, mxf4, nvf4mxf4) automatically.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">KernelSparseTmaWarpSpecialized2SmBlockScaledSm100</span></code>: Use 2 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma.sp</span></code> instruction and choose instr kind (mxf8f6f4, mxf4, nvf4mxf4) automatically.</p></li>
</ul>
<p>Similarly for epilogues, we can use <code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::collective::EpilogueScheduleAuto</span></code>.</p>
</section>
</section>
<section id="building-a-block-scaled-kernel">
<h2>Building a Block Scaled Kernel <a id="detailed_blockscale_example" name="detailed_blockscale_example"></a><a class="headerlink" href="#building-a-block-scaled-kernel" title="Link to this heading">#</a></h2>
<p>For non-blockscaled dense GEMM refer to <a class="reference internal" href="quickstart.html#instantiating-a-blackwell-sm100-gemm-kernel"><span class="std std-ref">quick start page</span></a>. An example dense GEMM can be found:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/70_blackwell_gemm/">Blackwell FP16 GEMM example</a>.</p></li>
</ol>
<p>An example sparse GEMM can be found:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/83_blackwell_sparse_gemm/">Blackwell FP16 Sparse GEMM example</a>.</p></li>
</ol>
<p>Narrow precision and block scaled narrow precision kernels can be built using CUTLASS 3.x collective builder interface
(as described in <a class="reference internal" href="gemm_api_3x.html#cutlass-30-gemm-api"><span class="std std-ref">CUTLASS 3.0 GEMM API</span></a>). However, special attention needs to be given to
A and B matrix layouts, alignment requirements, and dispatch policies to obtain a functionally correct and performant kernel
which are listed above.</p>
<p>Several examples of block scaled dense GEMM kernels can be found in <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/72_blackwell_narrow_precision_gemm/">examples/72_blackwell_narrow_precision_gemm</a> directory:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/72_blackwell_narrow_precision_gemm/72a_blackwell_nvfp4_bf16_gemm.cu">NVF4 Gemm with block scaling</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/72_blackwell_narrow_precision_gemm/72b_blackwell_nvfp4_nvfp4_gemm.cu">NVF4 Gemm with block scaling and NVF4 output matrix</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/72_blackwell_narrow_precision_gemm/72c_blackwell_mixed_mxfp8_bf16_gemm.cu">Mixed precision Nvf4 x Mxf8 GEMM with block scaling</a></p></li>
</ol>
<p>Several examples of block scaled sparse GEMM kernels can be found in <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/84_blackwell_narrow_precision_sparse_gemm">examples/84_blackwell_narrow_precision_sparse_gemm</a> directory:</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/84_blackwell_narrow_precision_sparse_gemm/84a_blackwell_nvfp4_bf16_sparse_gemm.cu">NVF4 Gemm with block scaling</a></p></li>
<li><p><a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/84_blackwell_narrow_precision_sparse_gemm/84b_blackwell_mixed_mxfp8_bf16_sparse_gemm.cu">Mixed precision Nvf4 x Mxf8 GEMM with block scaling</a></p></li>
</ol>
<p>Collective builder interface expects the same arguments as any other CUTLASS 3.x kernels as described
<a class="reference internal" href="gemm_api_3x.html#collective-builder-for-collectivemmas"><span class="std std-ref">here</span></a> with a small difference for Collective MMA builder interface.
As in all Blackwell kernels, the <code class="docutils literal notranslate"><span class="pre">TileShape_MNK</span></code> argument expects the <code class="docutils literal notranslate"><span class="pre">MmaTileShape_MNK</span></code> which is the tile shape needed
by 1 or 2 SM <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> instructions.</p>
<p>Let’s consider building a block scaled GEMM where the A matrix is of type <code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code> and column-major (N), and the
B matrix is of type <code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code> and row-major (T). We first need to describe the A and B tensors, and find the
instruction that can support the selected A and B type and layout pair. Then, we will choose the performance parameters.</p>
<p>The skeleton C++ code is shown below:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">///////////////////////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">//                Mainloop Builder Setup</span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////////////////////</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">// 1. Describe A and B tensors</span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementA</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutA</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementB</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutB</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>

<span class="w">  </span><span class="c1">// Mma&#39;s accumulator type</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementAccumulator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">float</span><span class="p">;</span><span class="w">           </span><span class="c1">// Always float for block scaled tcgen05.mma instructions</span>

<span class="w">  </span><span class="c1">//////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">// 2. Choose Performance Parameters</span>
<span class="w">  </span><span class="c1">//////////////////////////////////////////</span>

<span class="w">  </span><span class="c1">// Tile and cluster shapes</span>
<span class="w">  </span><span class="c1">// Collective MMA takes tile shape of the MMA operation as input</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">KernelMainloopPolicy</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">MmaTileShape_MNK</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ClusterShape_MNK</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="c1">// TBD</span>

<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CollectiveMainloop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">cutlass</span><span class="o">::</span><span class="n">gemm</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">CollectiveBuilder</span><span class="o">&lt;</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">Sm100</span><span class="p">,</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">OpClassBlockScaledTensorOp</span><span class="p">,</span><span class="w">      </span><span class="c1">// Arch and Tensorop spec</span>
<span class="w">      </span><span class="n">ElementA</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutA</span><span class="p">,</span><span class="w"> </span><span class="n">AlignA</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// A tensor elem type, layout and alignment requirement</span>
<span class="w">      </span><span class="n">ElementB</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutB</span><span class="p">,</span><span class="w"> </span><span class="n">AlignB</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// B tensor elem type, layout and alignment requirement</span>
<span class="w">      </span><span class="n">ElementAccumulator</span><span class="p">,</span><span class="w">                                                   </span><span class="c1">// Mma instruction accumulator type</span>
<span class="w">      </span><span class="n">MmaTileShape_MNK</span><span class="p">,</span><span class="w"> </span><span class="n">ClusterShape_MNK</span><span class="p">,</span><span class="w">                                   </span><span class="c1">// Mma instruction tile shape, cluster shape</span>
<span class="w">      </span><span class="c1">// Epilogue&#39;s SMEM usage that needs to be subtracted from overall SMEM capacity </span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">gemm</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">StageCountAutoCarveout</span><span class="o">&lt;</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="k">typename</span><span class="w"> </span><span class="nc">CollectiveEpilogue</span><span class="o">::</span><span class="n">SharedStorage</span><span class="p">))</span><span class="o">&gt;</span><span class="p">,</span>
<span class="w">      </span><span class="n">KernelMainloopPolicy</span><span class="w">                                                  </span><span class="c1">// Kernel schedule policy.</span>
<span class="w">                                                                            </span><span class="c1">// Auto or using targeted scheduling policy</span>
<span class="w">    </span><span class="o">&gt;::</span><span class="n">CollectiveOp</span><span class="p">;</span>
</pre></div>
</div>
<p>From the valid type and layout combinations <a class="reference internal" href="#bs_gemm_table"><span class="xref myst">Table 3</span></a>, we see that only <strong>row 3</strong> can support <code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code>x<code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code>
combination with NT layout. As a result, we need to use the <code class="docutils literal notranslate"><span class="pre">tcgen05.mma.kind:mxf8f6f4</span></code> instruction. Additionally, in order
to use <code class="docutils literal notranslate"><span class="pre">tcgen05.mma.kind:mxf8f6f4</span></code>, we see that A and B tensors both should be 128-element aligned.
Thus, we can describe A and B tensors as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">///////////////////////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">//                Mainloop Builder Setup</span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////////////////////</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">// 1. Describe A and B tensors</span>
<span class="w">  </span><span class="c1">///////////////////////////////////////////</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementA</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">mx_float4_t</span><span class="p">;</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignA</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">128</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutA</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">layout</span><span class="o">::</span><span class="n">ColumnMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementB</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="n">mx_float4_t</span><span class="p">;</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignB</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">128</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutB</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">layout</span><span class="o">::</span><span class="n">RowMajor</span><span class="p">;</span>
</pre></div>
</div>
<p>Next, we need to choose the performance parameters such as <code class="docutils literal notranslate"><span class="pre">MmaTileShape_MNK</span></code>, <code class="docutils literal notranslate"><span class="pre">KernelMainloopPolicy</span></code>,
and <code class="docutils literal notranslate"><span class="pre">ClusterShape_MNK</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">MmaTileShape_MNK</span></code> supported for <code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code>x<code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code> with <code class="docutils literal notranslate"><span class="pre">mxf8f6f4</span></code> are listed in <a class="reference internal" href="#bs_rows_3"><span class="xref myst">Table 11</span></a>.
For NT layout, we see that 3 <code class="docutils literal notranslate"><span class="pre">MmaTileShape_MNK</span></code> are supported: <code class="docutils literal notranslate"><span class="pre">128x128x128</span></code>, and <code class="docutils literal notranslate"><span class="pre">128x256x128</span></code> with 1SM instruction;
and <code class="docutils literal notranslate"><span class="pre">256x256x128</span></code> with 2SM instruction. Let’s say, we expect to get the best performance with <code class="docutils literal notranslate"><span class="pre">256x256x128</span></code> MMA tile shape
for our GEMM problem. Then, we need to set the <code class="docutils literal notranslate"><span class="pre">KernelMainloopPolicy</span></code> to <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span></code>.
Now, we need to choose the <code class="docutils literal notranslate"><span class="pre">ClusterShape_MNK</span></code>. Since we have selected a 2SM mma instruction, <code class="docutils literal notranslate"><span class="pre">ClusterShape_MNK</span></code> should be
compatible and its first mode should be a multiple of 2. <code class="docutils literal notranslate"><span class="pre">ClusterShape_MNK</span> <span class="pre">=</span> <span class="pre">cute::Shape&lt;_2,</span> <span class="pre">[_1|_2|_4],</span> <span class="pre">_1&gt;</span></code> or
<code class="docutils literal notranslate"><span class="pre">ClusterShape_MNK</span> <span class="pre">=</span> <span class="pre">cute::Shape&lt;_4,</span> <span class="pre">[_1|_2|_4],</span> <span class="pre">_1&gt;</span></code> would be valid options. Let’s choose <code class="docutils literal notranslate"><span class="pre">cute::Shape&lt;_4,_4,_1&gt;</span></code>.
Our performance parameters looks like below:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">//////////////////////////////////////////</span>
<span class="w">  </span><span class="c1">// 2. Choose Performance Parameters</span>
<span class="w">  </span><span class="c1">//////////////////////////////////////////</span>

<span class="w">  </span><span class="c1">// Tile and cluster shapes</span>
<span class="w">  </span><span class="c1">// Collective MMA takes tile shape of the MMA operation as input</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">KernelMainloopPolicy</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">gemm</span><span class="o">::</span><span class="n">KernelTmaWarpSpecialized2SmMxf8f6f4Sm100</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">MmaTileShape_MNK</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="n">cute</span><span class="o">::</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_256</span><span class="p">,</span><span class="n">_256</span><span class="p">,</span><span class="n">_128</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ClusterShape_MNK</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="n">cute</span><span class="o">::</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_4</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;</span><span class="p">;</span>
</pre></div>
</div>
<p>After we config the main-loop, let’s setup the epilogue.
A normal epilogue looks like below, we need to specify the output layout, datatype, alignment and PerSmTileShape_MNK, and let others to be default/auto.</p>
<p>PerSmTileShape_MNK should be deduced from the mainloop setup. For example, in above mainloop setup, the MmaTileShape_MNK is
256x256x128 and the KernelMainloopPolicy is 2sm policy.
It means each CTA is doing (256 / 2sm) x 256 x 128 output, so the PerSmTileShape_MNK is 128x256x128. The possible PerSmTileShape_MNK
is listed in <a class="reference internal" href="#epi_persmtileshape"><span class="xref myst">Table 15</span></a></p>
<p>The epilogue scheduling policy is configurable, and it is common to set <code class="docutils literal notranslate"><span class="pre">cutlass::epilogue::collective::EpilogueScheduleAuto</span></code>
to allow the epilogue builder to automatically select the appropriate policy. However, it can also be explicitly defined to
use other policies based on the 1sm or 2sm MMA instruction. The available policies are listed in <a class="reference internal" href="#epi_dispatch"><span class="xref myst">Table 14</span></a>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">// Describe C and D tensors</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">half_t</span><span class="p">;</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">8</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutC</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">layout</span><span class="o">::</span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementD</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">float_e2m1_t</span><span class="p">;</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">AlignD</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">32</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">GmemLayoutD</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">layout</span><span class="o">::</span><span class="n">RowMajor</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Mma&#39;s accumulator type</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementAccumulator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">float</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Epilogue computation&#39;s precision type</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">ElementCompute</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">float</span><span class="p">;</span>
<span class="w">  </span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// Construct CollectiveEpilogue</span>
<span class="w">  </span><span class="c1">//</span>

<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CollectiveEpilogue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">CollectiveBuilder</span><span class="o">&lt;</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">Sm100</span><span class="p">,</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">OpClassBlockScaledTensorOp</span><span class="p">,</span><span class="w">      </span><span class="c1">// Arch and Tensorop spec</span>
<span class="w">      </span><span class="n">MmaTileShape_MNK</span><span class="p">,</span><span class="w"> </span><span class="n">ClusterShape_MNK</span><span class="p">,</span><span class="w">                                   </span><span class="c1">// MMA tile shape, and cluster shape</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">EpilogueTileAuto</span><span class="p">,</span><span class="w">                      </span><span class="c1">// Epilogue subtile shape. Auto will find a suitable tile shape</span>
<span class="w">      </span><span class="n">ElementAccumulator</span><span class="p">,</span><span class="w"> </span><span class="n">ElementCompute</span><span class="p">,</span><span class="w">                                   </span><span class="c1">// Mma instr&#39;s accumulator type and compute precision for epilogue</span>
<span class="w">      </span><span class="n">ElementC</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutC</span><span class="p">,</span><span class="w"> </span><span class="n">AlignC</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// C tensor description</span>
<span class="w">      </span><span class="n">ElementD</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutD</span><span class="p">,</span><span class="w"> </span><span class="n">AlignD</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// D tensor description</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">TmaWarpSpecialized2Sm</span><span class="w">                              </span><span class="c1">// Epilogue schedule policy</span>
<span class="w">    </span><span class="o">&gt;::</span><span class="n">CollectiveOp</span><span class="p">;</span>
</pre></div>
</div>
<p>If we want to let the epilogue generate mxf4/nvf4/mxf6/mxf8 (i.e. elements + block-scalefactor), we need to setup the epilogue fusion into the builder.
First, we need to choose a SFDVectorSize indicates how many elements sharing the same block-scalefactor.
Then, we need to choose ElementSFD and GmemLayoutSFD which indicates the output datatype and which output-dim is used to generate the block-scalefactor.
Typically, GmemLayoutSFD would be same as the GmemLayoutD.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// Construct FusionOperation</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="k">constexpr</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">SFDVectorSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// Define the fusion operation applied during epilogue</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">FusionOperation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">fusion</span><span class="o">::</span><span class="n">LinCombBlockScaleFactor</span><span class="o">&lt;</span>
<span class="w">      </span><span class="n">SFDVectorSize</span><span class="p">,</span>
<span class="w">      </span><span class="n">ElementD</span><span class="p">,</span><span class="w"> </span><span class="n">ElementCompute</span><span class="p">,</span><span class="w"> </span>
<span class="w">      </span><span class="n">ElementSFD</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutSFD</span><span class="p">,</span>
<span class="w">      </span><span class="n">ElementC</span>
<span class="w">    </span><span class="o">&gt;</span><span class="p">;</span>

<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">CollectiveEpilogue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">CollectiveBuilder</span><span class="o">&lt;</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">Sm100</span><span class="p">,</span><span class="w"> </span><span class="n">cutlass</span><span class="o">::</span><span class="n">arch</span><span class="o">::</span><span class="n">OpClassBlockScaledTensorOp</span><span class="p">,</span><span class="w">      </span><span class="c1">// Arch and Tensorop spec</span>
<span class="w">      </span><span class="n">MmaTileShape_MNK</span><span class="p">,</span><span class="w"> </span><span class="n">ClusterShape_MNK</span><span class="p">,</span><span class="w">                                   </span><span class="c1">// MMA tile shape, and cluster shape</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">collective</span><span class="o">::</span><span class="n">EpilogueTileAuto</span><span class="p">,</span><span class="w">                      </span><span class="c1">// Epilogue subtile shape. Auto will find a suitable tile shape</span>
<span class="w">      </span><span class="n">ElementAccumulator</span><span class="p">,</span><span class="w"> </span><span class="n">ElementCompute</span><span class="p">,</span><span class="w">                                   </span><span class="c1">// Mma instr&#39;s accumulator type and compute precision for epilogue</span>
<span class="w">      </span><span class="n">ElementC</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutC</span><span class="p">,</span><span class="w"> </span><span class="n">AlignC</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// C tensor description</span>
<span class="w">      </span><span class="n">ElementD</span><span class="p">,</span><span class="w"> </span><span class="n">GmemLayoutD</span><span class="p">,</span><span class="w"> </span><span class="n">AlignD</span><span class="p">,</span><span class="w">                                        </span><span class="c1">// D tensor description</span>
<span class="w">      </span><span class="n">cutlass</span><span class="o">::</span><span class="n">epilogue</span><span class="o">::</span><span class="n">TmaWarpSpecialized2Sm</span><span class="w">                              </span><span class="c1">// Epilogue schedule policy</span>
<span class="w">      </span><span class="n">FusionOperation</span><span class="w">                                                       </span><span class="c1">// &lt;================================== Pass the fusion config into epilogue builder.</span>
<span class="w">    </span><span class="o">&gt;::</span><span class="n">CollectiveOp</span><span class="p">;</span>
</pre></div>
</div>
<p>Above example made a gentle introduction to using the fusion operations in the epilogue. For more detailed example, see
<a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/71_blackwell_gemm_with_collective_builder/71_blackwell_gemm_with_collective_builder.cu">Blackwell GEMM with collective builder</a></p>
<p>Note that we have first discussed the CollectiveMainloop, then the CollectiveEpilogue for clarity.
However, the CollectiveMainloop needs to know the SMEM utilization of the epilogue. Therefore, it needs to be setup before the CollectiveMainloop. See  <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/examples/72_blackwell_narrow_precision_gemm/">examples/72_blackwell_narrow_precision_gemm</a> directory for full kernel and run setup.</p>
<section id="scale-factor-layouts">
<h3>Scale Factor Layouts<a class="headerlink" href="#scale-factor-layouts" title="Link to this heading">#</a></h3>
<p>The scale factor layout consists of a 512B basic-block structure, as illustrated in the diagram below. Each block contains 128 M/N dimension and 4 scale factors (SF) along the K dimension.
The byte order of the basic storage chunk is row-major, meaning that M0SF0 to M0SF3, M32SF0 to M32SF3, M64SF0 to M64SF3, and M96SF0 to M96SF3 are stored consecutively in GMEM.</p>
<p><img alt="ALT" src="../../images/M128xK4_scalefactor_gmem.png" /></p>
<p>If the scale factor tensor exceeds M128xSF4, it indicates that there are multiple basic blocks along both the M and SFK dimensions. The arrangement of these basic blocks follows a K-major order. Here is a diagram illustrating the scenario where M equals 512 and the SFK is 16.</p>
<p><img alt="ALT" src="../../images/narrow_precison_multiple_block_sf_layout.png" /></p>
<p>The creation of scale factor tensors’ layouts are tedious. CUTLASS provides <code class="docutils literal notranslate"><span class="pre">Sm1xxBlockScaledConfig</span></code> to create these layouts easily
(See <a class="reference external" href="https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/detail/sm100_blockscaled_layout.hpp">sm100_blockscaled_layout.hpp</a>).
The interface to create SFA and SFB tensor layouts is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">auto</span><span class="w"> </span><span class="n">problem_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="n">M</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">K</span><span class="p">,</span><span class="w"> </span><span class="n">L</span><span class="p">);</span>
<span class="k">using</span><span class="w"> </span><span class="n">SfConfig</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sm1xxBlockScaledConfig</span><span class="o">&lt;</span><span class="n">SFVecSize</span><span class="o">&gt;</span><span class="p">;</span>

<span class="c1">// SFA shape: ((32,4), ceil(M/128)), ((SFVecSize,4), ceil(K/4), L)</span>
<span class="k">auto</span><span class="w"> </span><span class="n">layout_sfa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SfConfig</span><span class="o">::</span><span class="n">tile_atom_to_shape_SFA</span><span class="p">(</span><span class="n">problem_shape</span><span class="p">);</span>
<span class="c1">// SFB shape: ((32,4), ceil(N/128)), ((SFVecSize,4), ceil(K/4), L)</span>
<span class="k">auto</span><span class="w"> </span><span class="n">layout_sfb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SfConfig</span><span class="o">::</span><span class="n">tile_atom_to_shape_SFB</span><span class="p">(</span><span class="n">problem_shape</span><span class="p">);</span>

<span class="k">auto</span><span class="w"> </span><span class="n">tensor_sfa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">aptr</span><span class="p">,</span><span class="w"> </span><span class="n">layout_sfa</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tensor_sfb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">bptr</span><span class="p">,</span><span class="w"> </span><span class="n">layout_sfb</span><span class="p">);</span>
<span class="c1">// Access SF for for element m,k of A tensor</span>
<span class="k">auto</span><span class="w"> </span><span class="n">val_a_mk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor_sfa</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">));</span>
</pre></div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="blackwell-sm120-gemms">
<h1>Blackwell SM120 GEMMs<a class="headerlink" href="#blackwell-sm120-gemms" title="Link to this heading">#</a></h1>
<p>The NVIDIA RTX 5000 Series GPUs introduce support for new narrow precision (4bit and 6bit) block-scaled and non-block-scaled tensor cores. The PTX ISA has extended the <code class="docutils literal notranslate"><span class="pre">mma</span></code> instructions to support these data formats which are 1x to 4x faster than Ada architecture’s fp8 tensor cores. For more detailed information see <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#multiply-and-accumulate-instruction-mma"><code class="docutils literal notranslate"><span class="pre">mma</span></code> PTX documentation</a>.</p>
<p>CUTLASS 4.0 has added support for these newly introduced narrow precision GEMMs. Similar to the Blackwell SM100 GEMMs, the SM120 GEMMs can be built using the collective builder interface. See examples in <a class="reference internal" href="#../../examples/79_blackwell_geforce_gemm/"><span class="xref myst">examples/79_blackwell_geforce_gemm/</span></a> and unit tests listed below.</p>
<p>The data types supported and tensor alignment requirements are the same as the Blackwell SM100 GEMMs. The scale factor layout is also the same as SM100 mentioned above. <code class="docutils literal notranslate"><span class="pre">OpClassTensorOp</span></code> is used for non-blockscaled narrow precision GEMMs and <code class="docutils literal notranslate"><span class="pre">OpClassBlockScaledTensorOp</span></code> is used for blockscaled narrow precision GEMMs.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Ptx Instruction</p></th>
<th class="head"><p>Throughput</p></th>
<th class="head"><p>Notes</p></th>
<th class="head"><p>Unit Test</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mma.sync.aligned.kind::f8f6f4</p></td>
<td><p>1x Ada Fp8 Tensor Core(2x for FP32 accumulator)</p></td>
<td><p>Mixed precision MMA with A={f4,f6,f8} x B={f4,f6,f8} TN layouts</p></td>
<td><p><a class="reference internal" href="#../../test/unit/gemm/device/sm120_tensorop_gemm/"><span class="xref myst">unit test</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>mma.sync.aligned.kind::mxf8f6f4.block_scale</p></td>
<td><p>1x Ada Fp8 Tensor Core(2x for FP32 accumulator)</p></td>
<td><p>Block scaled mixed precision MMA with A={mxf4,mxf6,mxf8} x B={mxf4,mxf6,mxf8} with TN layouts</p></td>
<td><p><a class="reference internal" href="#../../test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/sm120_bs_gemm_mxf6_mxf8_f32_f32.cu"><span class="xref myst">unit test</span></a></p></td>
</tr>
<tr class="row-even"><td><p>mma.sync.aligned.kind::mxf4.block_scale</p></td>
<td><p>2x Ada Fp8 Tensor Core(4x for FP32 accumulator)</p></td>
<td><p>Block scaled MMA with A={mxf4} x B={mxf4} with TN layouts</p></td>
<td><p><a class="reference internal" href="#../../test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/sm120_bs_gemm_mxf4_mxf4_f32_f32.cu"><span class="xref myst">unit test</span></a></p></td>
</tr>
<tr class="row-odd"><td><p>mma.sync.aligned.kind::mxf4nvf4.block_scale.scale_vec::[2X|4X]</p></td>
<td><p>2x Ada Fp8 Tensor Core(4x for FP32 accumulator)</p></td>
<td><p>Block scaled MMA with A={mxf4} x B={mxf4} or A={nvf4} x B={nvf4} with TN layouts</p></td>
<td><p><a class="reference internal" href="#../../test/unit/gemm/device/sm120_blockscaled_tensorop_gemm/sm120_bs_gemm_nvf4_nvf4_f32_f32.cu"><span class="xref myst">unit test</span></a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Besides the similarities, there are some key differences from the Blackwell SM100 GEMMs:</p>
<section id="cluster-size">
<h2>Cluster Size<a class="headerlink" href="#cluster-size" title="Link to this heading">#</a></h2>
<p>On Geforce series graphics card, there is no multicast feature therefore the cluster shape is fixed to 1x1x1.</p>
</section>
<section id="tensor-layout">
<h2>Tensor Layout<a class="headerlink" href="#tensor-layout" title="Link to this heading">#</a></h2>
<p>Only TN layout is supported. Matrix A is row major and matrix B is column major.</p>
</section>
<section id="pingpong-v-s-cooperative-kernel-schedule">
<h2>Pingpong v.s. cooperative kernel schedule<a class="headerlink" href="#pingpong-v-s-cooperative-kernel-schedule" title="Link to this heading">#</a></h2>
<p>Similar to Hopper’s warp-group GEMM, SM120 GEMMs support both pingpong and cooperative kernel schedules. Pingpong kernel schedule has two groups of 4 MMA warps working on different output tiles, overlapping the mainloop and epilogue, while the cooperative kernel schedule has only one group of 8 MMA warps working on the same output tile. If <code class="docutils literal notranslate"><span class="pre">KernelScheduleAuto</span></code> is specified, <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code> will be selected by default.</p>
</section>
<section id="epilogue-schedule">
<h2>Epilogue schedule:<a class="headerlink" href="#epilogue-schedule" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">EpilogueScheduleAuto</span></code> must be used.</p>
</section>
<section id="tile-size">
<h2>Tile size:<a class="headerlink" href="#tile-size" title="Link to this heading">#</a></h2>
<p>Below are tables that summarize the valid tile shapes and dispatch policies for SM120 GEMMs. If the output is <code class="docutils literal notranslate"><span class="pre">float_6_t</span></code>, the tile size in the leading dimension of output tensor must be 128.</p>
<p><strong>Table 16: Valid Tile Shapes and Dispatch Policies for {float8_t, float_6_t, float_4_t} x {float8_t, float_6_t, float_4_t} of SM120 GEMMs</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>64x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>64x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-even"><td><p>128x64x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 17: Valid Tile Shapes for nv_float4_t x nv_float4_t of SM120 GEMMs</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-even"><td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 18: Valid Tile Shapes and Dispatch Policies for mx_float4_t x mx_float4_t of SM120 GEMMs</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
<tr class="row-even"><td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Table 19: Valid Tile Shapes and Dispatch Policies for mx_float4_t x mx_float4_t of SM120 GEMMs</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedMxf8f6f4Sm120</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpongMxf8f6f4Sm120</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>256x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedMxf8f6f4Sm120</span></code></p></td>
</tr>
<tr class="row-even"><td><p>128x128x256</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedMxf8f6f4Sm120</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpongMxf8f6f4Sm120</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>Specialized policies must be used to generate mixed-input-datatype <code class="docutils literal notranslate"><span class="pre">mx_float4_t</span></code> kernels.</p>
<p><strong>Table 20: Valid Tile Shapes and Dispatch Policies for {mx_float4_t, mx_float6_t, mx_float8_t} x {mx_float4_t, mx_float6_t, mx_float8_t}</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Mma Tile Shape</p></th>
<th class="head"><p>TN</p></th>
<th class="head"><p>TT</p></th>
<th class="head"><p>NT</p></th>
<th class="head"><p>NN</p></th>
<th class="head"><p>Dispatch Policy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128x128x128</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedPingpong</span></code> or <code class="docutils literal notranslate"><span class="pre">KernelTmaWarpSpecializedCooperative</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="copyright">
<h1>Copyright<a class="headerlink" href="#copyright" title="Link to this heading">#</a></h1>
<p>Copyright (c) 2025 - 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">Redistribution</span> <span class="ow">and</span> <span class="n">use</span> <span class="ow">in</span> <span class="n">source</span> <span class="ow">and</span> <span class="n">binary</span> <span class="n">forms</span><span class="p">,</span> <span class="k">with</span> <span class="ow">or</span> <span class="n">without</span>
  <span class="n">modification</span><span class="p">,</span> <span class="n">are</span> <span class="n">permitted</span> <span class="n">provided</span> <span class="n">that</span> <span class="n">the</span> <span class="n">following</span> <span class="n">conditions</span> <span class="n">are</span> <span class="n">met</span><span class="p">:</span>

  <span class="mf">1.</span> <span class="n">Redistributions</span> <span class="n">of</span> <span class="n">source</span> <span class="n">code</span> <span class="n">must</span> <span class="n">retain</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span> <span class="n">this</span>
  <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span><span class="o">.</span>

  <span class="mf">2.</span> <span class="n">Redistributions</span> <span class="ow">in</span> <span class="n">binary</span> <span class="n">form</span> <span class="n">must</span> <span class="n">reproduce</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span>
  <span class="n">this</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">documentation</span>
  <span class="ow">and</span><span class="o">/</span><span class="ow">or</span> <span class="n">other</span> <span class="n">materials</span> <span class="n">provided</span> <span class="k">with</span> <span class="n">the</span> <span class="n">distribution</span><span class="o">.</span>

  <span class="mf">3.</span> <span class="n">Neither</span> <span class="n">the</span> <span class="n">name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">copyright</span> <span class="n">holder</span> <span class="n">nor</span> <span class="n">the</span> <span class="n">names</span> <span class="n">of</span> <span class="n">its</span>
  <span class="n">contributors</span> <span class="n">may</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">endorse</span> <span class="ow">or</span> <span class="n">promote</span> <span class="n">products</span> <span class="n">derived</span> <span class="kn">from</span>
<span class="w">  </span><span class="nn">this</span> <span class="n">software</span> <span class="n">without</span> <span class="n">specific</span> <span class="n">prior</span> <span class="n">written</span> <span class="n">permission</span><span class="o">.</span>

  <span class="n">THIS</span> <span class="n">SOFTWARE</span> <span class="n">IS</span> <span class="n">PROVIDED</span> <span class="n">BY</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDERS</span> <span class="n">AND</span> <span class="n">CONTRIBUTORS</span> <span class="s2">&quot;AS IS&quot;</span>
  <span class="n">AND</span> <span class="n">ANY</span> <span class="n">EXPRESS</span> <span class="n">OR</span> <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span><span class="p">,</span> <span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">THE</span>
  <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span> <span class="n">OF</span> <span class="n">MERCHANTABILITY</span> <span class="n">AND</span> <span class="n">FITNESS</span> <span class="n">FOR</span> <span class="n">A</span> <span class="n">PARTICULAR</span> <span class="n">PURPOSE</span> <span class="n">ARE</span>
  <span class="n">DISCLAIMED</span><span class="o">.</span> <span class="n">IN</span> <span class="n">NO</span> <span class="n">EVENT</span> <span class="n">SHALL</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDER</span> <span class="n">OR</span> <span class="n">CONTRIBUTORS</span> <span class="n">BE</span> <span class="n">LIABLE</span>
  <span class="n">FOR</span> <span class="n">ANY</span> <span class="n">DIRECT</span><span class="p">,</span> <span class="n">INDIRECT</span><span class="p">,</span> <span class="n">INCIDENTAL</span><span class="p">,</span> <span class="n">SPECIAL</span><span class="p">,</span> <span class="n">EXEMPLARY</span><span class="p">,</span> <span class="n">OR</span> <span class="n">CONSEQUENTIAL</span>
  <span class="n">DAMAGES</span> <span class="p">(</span><span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">PROCUREMENT</span> <span class="n">OF</span> <span class="n">SUBSTITUTE</span> <span class="n">GOODS</span> <span class="n">OR</span>
  <span class="n">SERVICES</span><span class="p">;</span> <span class="n">LOSS</span> <span class="n">OF</span> <span class="n">USE</span><span class="p">,</span> <span class="n">DATA</span><span class="p">,</span> <span class="n">OR</span> <span class="n">PROFITS</span><span class="p">;</span> <span class="n">OR</span> <span class="n">BUSINESS</span> <span class="n">INTERRUPTION</span><span class="p">)</span> <span class="n">HOWEVER</span>
  <span class="n">CAUSED</span> <span class="n">AND</span> <span class="n">ON</span> <span class="n">ANY</span> <span class="n">THEORY</span> <span class="n">OF</span> <span class="n">LIABILITY</span><span class="p">,</span> <span class="n">WHETHER</span> <span class="n">IN</span> <span class="n">CONTRACT</span><span class="p">,</span> <span class="n">STRICT</span> <span class="n">LIABILITY</span><span class="p">,</span>
  <span class="n">OR</span> <span class="n">TORT</span> <span class="p">(</span><span class="n">INCLUDING</span> <span class="n">NEGLIGENCE</span> <span class="n">OR</span> <span class="n">OTHERWISE</span><span class="p">)</span> <span class="n">ARISING</span> <span class="n">IN</span> <span class="n">ANY</span> <span class="n">WAY</span> <span class="n">OUT</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">USE</span>
  <span class="n">OF</span> <span class="n">THIS</span> <span class="n">SOFTWARE</span><span class="p">,</span> <span class="n">EVEN</span> <span class="n">IF</span> <span class="n">ADVISED</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">POSSIBILITY</span> <span class="n">OF</span> <span class="n">SUCH</span> <span class="n">DAMAGE</span><span class="o">.</span>
</pre></div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>)</span>
<p>Only valid as scale factor data types.</p>
</aside>
</aside>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="grouped_scheduler.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">CUTLASS Grouped Kernel Schedulers</p>
      </div>
    </a>
    <a class="right-next"
       href="blackwell_cluster_launch_control.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Blackwell Cluster Launch Control</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Blackwell SM100 GEMMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-in-blackwell-sm100">New in Blackwell SM100</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#block-scaled-gemms">Block Scaled GEMMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#blackwell-narrow-precision-data-types">Blackwell Narrow Precision Data Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layouts-tensor-alignment-requirements-to-target-tcgen05-mma-instructions">Layouts, Tensor Alignment Requirements to Target <code class="docutils literal notranslate"><span class="pre">tcgen05.mma</span></code> Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mma-tile-shapes-supported">MMA tile shapes supported</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epilogue-config-supported">Epilogue config supported</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-kernel-dispatch-policies">Auto Kernel Dispatch Policies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-block-scaled-kernel">Building a Block Scaled Kernel <a id="detailed_blockscale_example" name="detailed_blockscale_example"></a></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scale-factor-layouts">Scale Factor Layouts</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#blackwell-sm120-gemms">Blackwell SM120 GEMMs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cluster-size">Cluster Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-layout">Tensor Layout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pingpong-v-s-cooperative-kernel-schedule">Pingpong v.s. cooperative kernel schedule</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epilogue-schedule">Epilogue schedule:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tile-size">Tile size:</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#copyright">Copyright</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
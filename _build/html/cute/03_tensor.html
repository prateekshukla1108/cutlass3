
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>CuTe Tensors &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cute/03_tensor';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="CuTe Tensor algorithms" href="04_algorithms.html" />
    <link rel="prev" title="CuTe Layout Algebra" href="02_layout_algebra.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../overview.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>



<li class="toctree-l1"><a class="reference internal" href="../terminology.html">CUTLASS Terminology</a></li>

<li class="toctree-l1"><a class="reference internal" href="../ide_setup.html">IDE Setup for CUTLASS Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../functionality.html">Functionality</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Core Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../fundamental_types.html">Fundamental Types</a></li>

<li class="toctree-l1"><a class="reference internal" href="../layout.html">Layouts and Tensors</a></li>



<li class="toctree-l1"><a class="reference internal" href="../tile_iterator_concept.html">Tile Iterator Concepts</a></li>

<li class="toctree-l1"><a class="reference internal" href="../pipeline.html">Synchronization primitives</a></li>

<li class="toctree-l1"><a class="reference internal" href="../code_organization.html">CUTLASS Code Organization</a></li>

<li class="toctree-l1"><a class="reference internal" href="../programming_guidelines.html">Programming Guidelines</a></li>

<li class="toctree-l1"><a class="reference internal" href="../utilities.html">CUTLASS Utilities</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Key Operations &amp; APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../gemm_api.html">CUTLASS GEMM API</a></li>



<li class="toctree-l1"><a class="reference internal" href="../gemm_api_3x.html">CUTLASS 3.0 GEMM API</a></li>



<li class="toctree-l1"><a class="reference internal" href="../efficient_gemm.html">Efficient GEMM in CUDA</a></li>


<li class="toctree-l1"><a class="reference internal" href="../implicit_gemm_convolution.html">CUTLASS Convolution</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CUTLASS 3.x Specifics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cutlass_3x_design.html">CUTLASS 3.0 Design</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cutlass_3x_backwards_compatibility.html">CUTLASS 3.0 GEMM Backwards Compatibility</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CUTE - Compositional Universal Tile Engine</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="00_quickstart.html">Getting Started With CuTe</a></li>
<li class="toctree-l1"><a class="reference internal" href="01_layout.html">CuTe Layouts</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_layout_algebra.html">CuTe Layout Algebra</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">CuTe Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_algorithms.html">CuTe Tensor algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="0t_mma_atom.html">CuTe’s support for Matrix Multiply-Accumulate instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="0x_gemm_tutorial.html">CuTe dense matrix-matrix multiply tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="0y_predication.html">Predication: What to do when tiling isn’t perfect</a></li>
<li class="toctree-l1"><a class="reference internal" href="0z_tma_tensors.html">CuTe TMA Tensors</a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Features &amp; Platform Specifics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dependent_kernel_launch.html">Dependent kernel launches</a></li>
<li class="toctree-l1"><a class="reference internal" href="../grouped_scheduler.html">CUTLASS Grouped Kernel Schedulers</a></li>





<li class="toctree-l1"><a class="reference internal" href="../blackwell_functionality.html">Blackwell SM100 GEMMs</a></li>


<li class="toctree-l1"><a class="reference internal" href="../blackwell_cluster_launch_control.html">Blackwell Cluster Launch Control</a></li>

<li class="toctree-l1"><a class="reference internal" href="../profiler.html">CUTLASS Profiler</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Building CUTLASS</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../build/building_in_windows_with_visual_studio.html">Building on Windows with Visual Studio</a></li>





<li class="toctree-l1"><a class="reference internal" href="../build/building_with_clang_as_host_compiler.html">Building with Clang as host compiler</a></li>


</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcute/03_tensor.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/cute/03_tensor.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>CuTe Tensors</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-operations">Fundamental operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-engines">Tensor Engines</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tagged-iterators">Tagged Iterators</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-creation">Tensor Creation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonowning-tensors">Nonowning Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#owning-tensors">Owning Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-a-tensor">Accessing a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tiling-a-tensor">Tiling a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slicing-a-tensor">Slicing a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioning-a-tensor">Partitioning a Tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-and-outer-partitioning">Inner and outer partitioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thread-value-partitioning">Thread-Value partitioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#copy-a-subtile-from-global-memory-to-registers">Copy a subtile from global memory to registers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#copyright">Copyright</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="cute-tensors">
<h1>CuTe Tensors<a class="headerlink" href="#cute-tensors" title="Link to this heading">#</a></h1>
<p>This document describes <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, CuTe’s core container that deploys the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> concepts previously described.</p>
<p>Fundamentally, a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> represents a multidimensional array. <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s abstracts away the details of how the array’s elements are organized and how the array’s elements are stored. This lets users write algorithms that access multidimensional arrays generically and potentially specialize algorithms on a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s traits. For example, the rank of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> can be dispatched against, the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> of data can be inspected, and the type of data can be verified.</p>
<p>A <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is represented by two template parameters: <code class="docutils literal notranslate"><span class="pre">Engine</span></code> and <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.
For a description of <code class="docutils literal notranslate"><span class="pre">Layout</span></code>, please refer to <a class="reference internal" href="01_layout.html"><span class="std std-doc">the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> section</span></a>.
The <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> presents the same shape and access operators as the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> and uses the result of the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> computation to
offset and dereference a random-access iterator held by the <code class="docutils literal notranslate"><span class="pre">Engine</span></code>.
That is, the layout of the data is provided by <code class="docutils literal notranslate"><span class="pre">Layout</span></code> and the actual data is provided by the iterator. Such data can live in any kind of memory – global memory, shared memory, register memory – or can even be transformed or generated on the fly.</p>
<section id="fundamental-operations">
<h2>Fundamental operations<a class="headerlink" href="#fundamental-operations" title="Link to this heading">#</a></h2>
<p>CuTe <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> provides container-like operations for accessing elements.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.data()</span></code>. The iterator this <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> holds.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.size()</span></code>. The total logical size of this <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.operator[](Coord)</span></code>. Access the element corresponding to the logical coordinate <code class="docutils literal notranslate"><span class="pre">Coord</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.operator()(Coord)</span></code>. Access the element corresponding to the logical coordinate <code class="docutils literal notranslate"><span class="pre">Coord</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.operator()(Coords...)</span></code>. Access the element corresponding to the logical coordinate <code class="docutils literal notranslate"><span class="pre">make_coord(Coords...)</span></code>.</p></li>
</ul>
<p>CuTe <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> provides a similar core of hierarchical operations as <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rank&lt;I...&gt;(Tensor)</span></code>. The rank of the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">depth&lt;I...&gt;(Tensor)</span></code>. The depth of the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shape&lt;I...&gt;(Tensor)</span></code>. The shape of the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size&lt;I...&gt;(Tensor)</span></code>. The size of the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">layout&lt;I...&gt;(Tensor)</span></code>. The layout of the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensor&lt;I...&gt;(Tensor)</span></code>. The subtensor corresponding to the the <code class="docutils literal notranslate"><span class="pre">I...</span></code>th mode of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p></li>
</ul>
</section>
<section id="tensor-engines">
<h2>Tensor Engines<a class="headerlink" href="#tensor-engines" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Engine</span></code> concept is a wrapper for an iterator or array of data.
It uses a stripped-down interface of <code class="docutils literal notranslate"><span class="pre">std::array</span></code> to present the iterator.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">using</span><span class="w"> </span><span class="n">iterator</span><span class="w">     </span><span class="o">=</span><span class="w">  </span><span class="c1">// The iterator type</span>
<span class="k">using</span><span class="w"> </span><span class="n">value_type</span><span class="w">   </span><span class="o">=</span><span class="w">  </span><span class="c1">// The iterator value-type</span>
<span class="k">using</span><span class="w"> </span><span class="n">reference</span><span class="w">    </span><span class="o">=</span><span class="w">  </span><span class="c1">// The iterator reference-type</span>
<span class="n">iterator</span><span class="w"> </span><span class="n">begin</span><span class="p">()</span><span class="w">      </span><span class="c1">// The iterator</span>
</pre></div>
</div>
<p>In general, users do not need to construct <code class="docutils literal notranslate"><span class="pre">Engine</span></code>s on their own. When a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is constructed,
the appropriate engine – often <code class="docutils literal notranslate"><span class="pre">ArrayEngine&lt;T,N&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">ViewEngine&lt;Iter&gt;</span></code>, or
<code class="docutils literal notranslate"><span class="pre">ConstViewEngine&lt;Iter&gt;</span></code> – will be constructed.</p>
<section id="tagged-iterators">
<h3>Tagged Iterators<a class="headerlink" href="#tagged-iterators" title="Link to this heading">#</a></h3>
<p>Any random-access iterator can be used to construct a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, but
users can also “tag” any iterator with a memory space –
e.g., to indicate this iterator is accessing global memory or shared memory.
This is done by calling <code class="docutils literal notranslate"><span class="pre">make_gmem_ptr(g)</span></code> or <code class="docutils literal notranslate"><span class="pre">make_gmem_ptr&lt;T&gt;(g)</span></code> to tag <code class="docutils literal notranslate"><span class="pre">g</span></code> as a global memory iterator,
and <code class="docutils literal notranslate"><span class="pre">make_smem_ptr(s)</span></code> or <code class="docutils literal notranslate"><span class="pre">make_smem_ptr&lt;T&gt;(s)</span></code> to tag <code class="docutils literal notranslate"><span class="pre">s</span></code> as a shared memory iterator.</p>
<p>Tagging memory makes it possible for CuTe’s <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> algorithms
to use the fastest implementation for the specific kind(s) of memory.
When calling very specific operations with <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s, it also allows those
operators to verify the tags against what is expected.
For example, some kinds of optimized copy operations require
the source of the copy to be global memory
and the destination of the copy to be shared memory.
Tagging makes it possible for CuTe to dispatch
to those copy operations and/or verify against those copy operations.</p>
</section>
</section>
<section id="tensor-creation">
<h2>Tensor Creation<a class="headerlink" href="#tensor-creation" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s can be constructed as owning or nonowning.</p>
<p>“Owning” <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s behave like <code class="docutils literal notranslate"><span class="pre">std::array</span></code>.
When you copy the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, you (deep-)copy its elements,
and the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s destructor deallocates the array of elements.</p>
<p>“Nonowning” <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s behave like a (raw) pointer.
Copying the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> doesn’t copy the elements,
and destroying the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> doesn’t deallocate the array of elements.</p>
<p>This has implications for developers of generic <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> algorithms.
For example, input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> parameters of a function
should be passed by referece or const reference,
because passing a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> by value
may or may not make a deep copy of the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s elements.</p>
<section id="nonowning-tensors">
<h3>Nonowning Tensors<a class="headerlink" href="#nonowning-tensors" title="Link to this heading">#</a></h3>
<p>A <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is usually a nonowning view of existing memory.
Nonowning <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s are created by calling <code class="docutils literal notranslate"><span class="pre">make_tensor</span></code>
with two arguments: a random-access iterator, and the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> or arguments to construct a <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.</p>
<p>Here are some examples of creating <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s
that are nonowning views of existing memory.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">...;</span>

<span class="c1">// Untagged pointers</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">tensor_8</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">make_layout</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{}));</span><span class="w">  </span><span class="c1">// Construct with Layout</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">tensor_8s</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{});</span><span class="w">               </span><span class="c1">// Construct with Shape</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">tensor_8d2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">);</span><span class="w">                   </span><span class="c1">// Construct with Shape and Stride</span>

<span class="c1">// Global memory (static or dynamic layouts)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">gmem_8s</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_gmem_ptr</span><span class="p">(</span><span class="n">A</span><span class="p">),</span><span class="w"> </span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">gmem_8d</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_gmem_ptr</span><span class="p">(</span><span class="n">A</span><span class="p">),</span><span class="w"> </span><span class="mi">8</span><span class="p">);</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">gmem_8sx16d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_gmem_ptr</span><span class="p">(</span><span class="n">A</span><span class="p">),</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{},</span><span class="mi">16</span><span class="p">));</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">gmem_8dx16s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_gmem_ptr</span><span class="p">(</span><span class="n">A</span><span class="p">),</span><span class="w"> </span><span class="n">make_shape</span><span class="w"> </span><span class="p">(</span><span class="w">      </span><span class="mi">8</span><span class="w">  </span><span class="p">,</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">16</span><span class="o">&gt;</span><span class="p">{}),</span>
<span class="w">                                                   </span><span class="n">make_stride</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">16</span><span class="o">&gt;</span><span class="p">{},</span><span class="n">Int</span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="o">&gt;</span><span class="p">{}));</span>

<span class="c1">// Shared memory (static or dynamic layouts)</span>
<span class="n">Layout</span><span class="w"> </span><span class="n">smem_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_layout</span><span class="p">(</span><span class="n">make_shape</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">4</span><span class="o">&gt;</span><span class="p">{},</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{}));</span>
<span class="n">__shared__</span><span class="w"> </span><span class="kt">float</span><span class="w"> </span><span class="n">smem</span><span class="p">[</span><span class="k">decltype</span><span class="p">(</span><span class="n">cosize</span><span class="p">(</span><span class="n">smem_layout</span><span class="p">))</span><span class="o">::</span><span class="n">value</span><span class="p">];</span><span class="w">   </span><span class="c1">// (static-only allocation)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">smem_4x8_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_smem_ptr</span><span class="p">(</span><span class="n">smem</span><span class="p">),</span><span class="w"> </span><span class="n">smem_layout</span><span class="p">);</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">smem_4x8_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">make_smem_ptr</span><span class="p">(</span><span class="n">smem</span><span class="p">),</span><span class="w"> </span><span class="n">shape</span><span class="p">(</span><span class="n">smem_layout</span><span class="p">),</span><span class="w"> </span><span class="n">LayoutRight</span><span class="p">{});</span>
</pre></div>
</div>
<p>As shown, users wrap the pointer by identifying its memory space:
e.g., global memory (via <code class="docutils literal notranslate"><span class="pre">make_gmem_ptr</span></code> or <code class="docutils literal notranslate"><span class="pre">make_gmem_ptr&lt;T&gt;</span></code>) or shared memory (via <code class="docutils literal notranslate"><span class="pre">make_smem_ptr</span></code> or <code class="docutils literal notranslate"><span class="pre">make_smem_ptr&lt;T&gt;</span></code>).
<code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s that view existing memory can have either static or dynamic <code class="docutils literal notranslate"><span class="pre">Layout</span></code>s.</p>
<p>Calling <code class="docutils literal notranslate"><span class="pre">print</span></code> on all of the above tensors displays</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor_8</span>     <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="n">_8</span><span class="p">:</span><span class="n">_1</span>
<span class="n">tensor_8s</span>    <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="n">_8</span><span class="p">:</span><span class="n">_1</span>
<span class="n">tensor_8d2</span>   <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="mi">8</span><span class="p">:</span><span class="mi">2</span>
<span class="n">gmem_8s</span>      <span class="p">:</span> <span class="n">gmem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="n">_8</span><span class="p">:</span><span class="n">_1</span>
<span class="n">gmem_8d</span>      <span class="p">:</span> <span class="n">gmem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="mi">8</span><span class="p">:</span><span class="n">_1</span>
<span class="n">gmem_8sx16d</span>  <span class="p">:</span> <span class="n">gmem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_8</span><span class="p">,</span><span class="mi">16</span><span class="p">):(</span><span class="n">_1</span><span class="p">,</span><span class="n">_8</span><span class="p">)</span>
<span class="n">gmem_8dx16s</span>  <span class="p">:</span> <span class="n">gmem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f42efc00000</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="n">_16</span><span class="p">):(</span><span class="n">_16</span><span class="p">,</span><span class="n">_1</span><span class="p">)</span>
<span class="n">smem_4x8_col</span> <span class="p">:</span> <span class="n">smem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f4316000000</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_1</span><span class="p">,</span><span class="n">_4</span><span class="p">)</span>
<span class="n">smem_4x8_row</span> <span class="p">:</span> <span class="n">smem_ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7f4316000000</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_8</span><span class="p">,</span><span class="n">_1</span><span class="p">)</span>
</pre></div>
</div>
<p>which displays the pointer type along with any memory space tags, the pointer’s <code class="docutils literal notranslate"><span class="pre">value_type</span></code> width, the raw pointer address, and the associated <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.</p>
</section>
<section id="owning-tensors">
<h3>Owning Tensors<a class="headerlink" href="#owning-tensors" title="Link to this heading">#</a></h3>
<p>A <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> can also be an owning array of memory.
Owning <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s are created by calling <code class="docutils literal notranslate"><span class="pre">make_tensor&lt;T&gt;</span></code>,
where <code class="docutils literal notranslate"><span class="pre">T</span></code> is the type of each element of the array, and
a <code class="docutils literal notranslate"><span class="pre">Layout</span></code> or arguments to construct a <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.
The array is allocated analogously to <code class="docutils literal notranslate"><span class="pre">std::array&lt;T,N&gt;</span></code> and, therefore, owning <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s must be constructed with a <code class="docutils literal notranslate"><span class="pre">Layout</span></code> that has static shapes and static strides.
CuTe does not perform dynamic memory allocation in <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s as it is not a common or performant operation within CUDA kernels.</p>
<p>Here are some examples of creating owning <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Register memory (static layouts only)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem_4x8_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem_4x8_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="o">&gt;</span><span class="p">{},</span>
<span class="w">                                         </span><span class="n">LayoutRight</span><span class="p">{});</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem_4x8_pad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="w"> </span><span class="n">_8</span><span class="o">&gt;</span><span class="p">{},</span>
<span class="w">                                         </span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">_32</span><span class="p">,</span><span class="n">_2</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem_4x8_like</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor_like</span><span class="p">(</span><span class="n">rmem_4x8_pad</span><span class="p">);</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">make_tensor_like</span></code> function makes an owning Tensor of register memory with the same value type and shape as its input <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> argument and attempts to use the same order of strides as well.</p>
<p>Calling <code class="docutils literal notranslate"><span class="pre">print</span></code> on each of the above tensors produces similar output</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rmem_4x8_col</span>  <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7fff48929460</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_1</span><span class="p">,</span><span class="n">_4</span><span class="p">)</span>
<span class="n">rmem_4x8_row</span>  <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7fff489294e0</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_8</span><span class="p">,</span><span class="n">_1</span><span class="p">)</span>
<span class="n">rmem_4x8_pad</span>  <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7fff489295e0</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_32</span><span class="p">,</span><span class="n">_2</span><span class="p">)</span>
<span class="n">rmem_4x8_like</span> <span class="p">:</span> <span class="n">ptr</span><span class="p">[</span><span class="mi">32</span><span class="n">b</span><span class="p">](</span><span class="mh">0x7fff48929560</span><span class="p">)</span> <span class="n">o</span> <span class="p">(</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="p">):(</span><span class="n">_8</span><span class="p">,</span><span class="n">_1</span><span class="p">)</span>
</pre></div>
</div>
<p>and we can see that each pointer address is unique indicating that each <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is a unique array-like allocation.</p>
</section>
</section>
<section id="accessing-a-tensor">
<h2>Accessing a Tensor<a class="headerlink" href="#accessing-a-tensor" title="Link to this heading">#</a></h2>
<p>Users can access the elements of a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> via <code class="docutils literal notranslate"><span class="pre">operator()</span></code> and <code class="docutils literal notranslate"><span class="pre">operator[]</span></code>,
which take <code class="docutils literal notranslate"><span class="pre">IntTuple</span></code>s of logical coordinates.</p>
<p>When users access a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>,
the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> uses its <code class="docutils literal notranslate"><span class="pre">Layout</span></code> to map the logical coordinate
to an offset that can be accessed by the iterator.
You can see this in <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>’s implementation of <code class="docutils literal notranslate"><span class="pre">operator[]</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">Coord</span><span class="o">&gt;</span>
<span class="k">decltype</span><span class="p">(</span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="k">operator</span><span class="p">[](</span><span class="n">Coord</span><span class="w"> </span><span class="k">const</span><span class="o">&amp;</span><span class="w"> </span><span class="n">coord</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">data</span><span class="p">()[</span><span class="n">layout</span><span class="p">()(</span><span class="n">coord</span><span class="p">)];</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For example, we can read and write to <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s using natural coordinates, using the variadic <code class="docutils literal notranslate"><span class="pre">operator()</span></code>, or the container-like <code class="docutils literal notranslate"><span class="pre">operator[]</span></code>.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">_4</span><span class="p">,</span><span class="n">_5</span><span class="o">&gt;</span><span class="p">,</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">13</span><span class="o">&gt;&gt;</span><span class="p">{},</span>
<span class="w">                              </span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">_12</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;</span><span class="p">,</span><span class="w">    </span><span class="n">_64</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">b_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">...;</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">b_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">));</span>

<span class="c1">// Fill A via natural coordinates op[]</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">m0</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">m0</span><span class="p">)</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">m1</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">m1</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">n</span><span class="p">)</span>
<span class="w">      </span><span class="n">A</span><span class="p">[</span><span class="n">make_coord</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="n">m0</span><span class="p">,</span><span class="n">m1</span><span class="p">),</span><span class="n">n</span><span class="p">)]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">m0</span><span class="p">;</span>

<span class="c1">// Transpose A into B using variadic op()</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">m</span><span class="p">)</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">n</span><span class="p">)</span>
<span class="w">    </span><span class="n">B</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">);</span>

<span class="c1">// Copy B to A as if they are arrays</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span>
<span class="w">  </span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
</pre></div>
</div>
</section>
<section id="tiling-a-tensor">
<h2>Tiling a Tensor<a class="headerlink" href="#tiling-a-tensor" title="Link to this heading">#</a></h2>
<p>Many of the <a class="reference external" href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md"><code class="docutils literal notranslate"><span class="pre">Layout</span></code> algebra operations</a> can also be applied to <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="w">   </span><span class="n">composition</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">Tiler</span><span class="p">)</span>
<span class="n">logical_divide</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">Tiler</span><span class="p">)</span>
<span class="w"> </span><span class="n">zipped_divide</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">Tiler</span><span class="p">)</span>
<span class="w">  </span><span class="n">tiled_divide</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">Tiler</span><span class="p">)</span>
<span class="w">   </span><span class="n">flat_divide</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span><span class="w"> </span><span class="n">Tiler</span><span class="p">)</span>
</pre></div>
</div>
<p>The above operations allows arbitrary subtensors to be “factored out” of <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s. This very commonly used in tiling for threadgroups, tiling for MMAs, and reodering tiles of data for threads.</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">_product</span></code> operations are not implemented for <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s as those would
often produce layouts with increased codomain sizes, which means the <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> would
require accessing elements unpredictably far outside its previous bounds. <code class="docutils literal notranslate"><span class="pre">Layout</span></code>s can be
used in products, but not <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>s.</p>
</section>
<section id="slicing-a-tensor">
<h2>Slicing a Tensor<a class="headerlink" href="#slicing-a-tensor" title="Link to this heading">#</a></h2>
<p>Whereas accessing a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> with a coordinate will return an element of that tensor,
slicing a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> will return a subtensor of all the elements in the sliced mode(s).</p>
<p>Slices are performed through the same <code class="docutils literal notranslate"><span class="pre">operator()</span></code>
that are used for accessing an individual element.
Passing in <code class="docutils literal notranslate"><span class="pre">_</span></code> (the underscore character, an instance of the <code class="docutils literal notranslate"><span class="pre">cute::Underscore</span></code> type)
has the same effect as <code class="docutils literal notranslate"><span class="pre">:</span></code> (the colon character) in Fortran or Matlab:
retain that mode of the tensor as if no coordinate had been used.</p>
<p>Slicing a tensor performs two operations,</p>
<ul class="simple">
<li><p>the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> is evaluated on the partial coordinate and the resulting offset is accumulated into the iterator – the new iterator points to the start of the new tensor.</p></li>
<li><p>the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> modes cooresponding to <code class="docutils literal notranslate"><span class="pre">_</span></code>-elements of the coordinate are used to construct a new layout.
Together, the new iterator and the new layout construct the new tensor.</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// ((_3,2),(2,_5,_2)):((4,1),(_2,13,100))</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">make_shape</span><span class="w"> </span><span class="p">(</span><span class="n">make_shape</span><span class="w"> </span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">3</span><span class="o">&gt;</span><span class="p">{},</span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="n">make_shape</span><span class="w"> </span><span class="p">(</span><span class="w">       </span><span class="mi">2</span><span class="p">,</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">5</span><span class="o">&gt;</span><span class="p">{},</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">{})),</span>
<span class="w">                            </span><span class="n">make_stride</span><span class="p">(</span><span class="n">make_stride</span><span class="p">(</span><span class="w">       </span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">make_stride</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">{},</span><span class="w">      </span><span class="mi">13</span><span class="p">,</span><span class="w">     </span><span class="mi">100</span><span class="p">)));</span>

<span class="c1">// ((2,_5,_2)):((_2,13,100))</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">_</span><span class="p">);</span>

<span class="c1">// ((_3,_2)):((4,1))</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span>

<span class="c1">// (_3,2):(4,1)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">D</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">),</span><span class="mi">5</span><span class="p">);</span>

<span class="c1">// (_3,_5):(4,13)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">E</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="n">make_coord</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>

<span class="c1">// (2,2,_2):(1,_2,100)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">F</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">A</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">_</span><span class="p">),</span><span class="n">make_coord</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="n">_</span><span class="p">));</span>
</pre></div>
</div>
<p align="center">
  <img src="../../../images/cute/slice.png" alt="slice.png" height="300"/>
</p>
<p>In the image above, a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is sliced in various ways and the subtensors generated by those slices are highlighted within the original tensor. Note that tensor <code class="docutils literal notranslate"><span class="pre">C</span></code> and <code class="docutils literal notranslate"><span class="pre">D</span></code> contain the same elements, but have different ranks and shapes due to the use of <code class="docutils literal notranslate"><span class="pre">_</span></code> versus the use of <code class="docutils literal notranslate"><span class="pre">make_coord(_,_)</span></code>. In each case, the rank of the result is equal to the number of <code class="docutils literal notranslate"><span class="pre">Underscore</span></code>s in the slicing coordinate.</p>
</section>
<section id="partitioning-a-tensor">
<h2>Partitioning a Tensor<a class="headerlink" href="#partitioning-a-tensor" title="Link to this heading">#</a></h2>
<p>To implement generic partitioning of a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, we apply composition or tiling followed by a slicing. This can be performed in many ways, but we have found three ways that are particularly useful: inner-partitioning, outer-partitioning, and TV-layout-partitioning.</p>
<section id="inner-and-outer-partitioning">
<h3>Inner and outer partitioning<a class="headerlink" href="#inner-and-outer-partitioning" title="Link to this heading">#</a></h3>
<p>Let’s take a tiled example and look at how we can slice it in useful ways.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">24</span><span class="p">));</span><span class="w">  </span><span class="c1">// (8,24)</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tiler</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="o">&gt;</span><span class="p">{};</span><span class="w">                    </span><span class="c1">// (_4,_8)</span>

<span class="n">Tensor</span><span class="w"> </span><span class="n">tiled_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">zipped_divide</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">tiler</span><span class="p">);</span><span class="w">       </span><span class="c1">// ((_4,_8),(2,3))</span>
</pre></div>
</div>
<p>Suppose that we want to give each threadgroup one of these 4x8 tiles of data. Then we can use our threadgroup coordinate to index into the second mode.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">cta_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tiled_a</span><span class="p">(</span><span class="n">make_coord</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">),</span><span class="w"> </span><span class="n">make_coord</span><span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">));</span><span class="w">  </span><span class="c1">// (_4,_8)</span>
</pre></div>
</div>
<p>We call this an <em>inner-partition</em> because it keeps the inner “tile” mode. This pattern of applying a tiler and then slicing out that tile by indexing into the remainder mode is common and has been wrapped into its own function <code class="docutils literal notranslate"><span class="pre">inner_partition(Tensor,</span> <span class="pre">Tiler,</span> <span class="pre">Coord)</span></code>. You’ll often see <code class="docutils literal notranslate"><span class="pre">local_tile(Tensor,</span> <span class="pre">Tiler,</span> <span class="pre">Coord)</span></code> which is just another name for <code class="docutils literal notranslate"><span class="pre">inner_partition</span></code>. The <code class="docutils literal notranslate"><span class="pre">local_tile</span></code> partitioner is very often applied at the threadgroup level to partition tensors into tiles across threadgroups.</p>
<p>Alternatively, suppose that we have 32 threads and want to give each thread one element of these 4x8 tiles of data. Then we can use our thread to index into the first mode.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">thr_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tiled_a</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">make_coord</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="n">_</span><span class="p">));</span><span class="w"> </span><span class="c1">// (2,3)</span>
</pre></div>
</div>
<p>We call this an <em>outer-partition</em> because it keeps the outer “rest” mode. This pattern of applying a tiler and then slicing into that tile by indexing into the tile mode is common and has been wrapped into its own function <code class="docutils literal notranslate"><span class="pre">outer_partition(Tensor,</span> <span class="pre">Tiler,</span> <span class="pre">Coord)</span></code>. Sometimes you’ll see <code class="docutils literal notranslate"><span class="pre">local_partition(Tensor,</span> <span class="pre">Layout,</span> <span class="pre">Idx)</span></code>, which is a rank-sensitive wrapper around <code class="docutils literal notranslate"><span class="pre">outer_partition</span></code> that transforms the <code class="docutils literal notranslate"><span class="pre">Idx</span></code> into a <code class="docutils literal notranslate"><span class="pre">Coord</span></code> using the inverse of the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> and then constructs a <code class="docutils literal notranslate"><span class="pre">Tiler</span></code> with the same top-level shape of the <code class="docutils literal notranslate"><span class="pre">Layout</span></code>. This allows the user to ask for a row-major, column-major, or arbitrary layout of threads with a given shape that can be used to partition into a tensor.</p>
<p>To see how these partitioning patterns are used, see the <a class="reference internal" href="0x_gemm_tutorial.html"><span class="std std-doc">introductory GEMM tutorial</span></a>.</p>
</section>
<section id="thread-value-partitioning">
<h3>Thread-Value partitioning<a class="headerlink" href="#thread-value-partitioning" title="Link to this heading">#</a></h3>
<p>Another common partitioning strategy is called a thread-value partitioning. In this pattern, we construct a <code class="docutils literal notranslate"><span class="pre">Layout</span></code> that represents the mapping of all threads (or any parallel agent) and all values that each thread will receive to coordinates of the target data. With <code class="docutils literal notranslate"><span class="pre">composition</span></code> the target data layout is transformed according to our TV-layout and then we can simply slice into the thread-mode of the result with our thread index.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Construct a TV-layout that maps 8 thread indices and 4 value indices</span>
<span class="c1">//   to 1D coordinates within a 4x8 tensor</span>
<span class="c1">// (T8,V4) -&gt; (M4,N8)</span>
<span class="k">auto</span><span class="w"> </span><span class="n">tv_layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Layout</span><span class="o">&lt;</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_2</span><span class="p">,</span><span class="n">_4</span><span class="o">&gt;</span><span class="p">,</span><span class="n">Shape</span><span class="w"> </span><span class="o">&lt;</span><span class="n">_2</span><span class="p">,</span><span class="w"> </span><span class="n">_2</span><span class="o">&gt;&gt;</span><span class="p">,</span>
<span class="w">                        </span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">_8</span><span class="p">,</span><span class="n">_1</span><span class="o">&gt;</span><span class="p">,</span><span class="n">Stride</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_16</span><span class="o">&gt;&gt;&gt;</span><span class="p">{};</span><span class="w"> </span><span class="c1">// (8,4)</span>

<span class="c1">// Construct a 4x8 tensor with any layout</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_4</span><span class="p">,</span><span class="n">_8</span><span class="o">&gt;</span><span class="p">{},</span><span class="w"> </span><span class="n">LayoutRight</span><span class="p">{});</span><span class="w">    </span><span class="c1">// (4,8)</span>
<span class="c1">// Compose A with the tv_layout to transform its shape and order</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">tv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">composition</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">tv_layout</span><span class="p">);</span><span class="w">                           </span><span class="c1">// (8,4)</span>
<span class="c1">// Slice so each thread has 4 values in the shape and order that the tv_layout prescribes</span>
<span class="n">Tensor</span><span class="w">  </span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tv</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">_</span><span class="p">);</span><span class="w">                                  </span><span class="c1">// (4)</span>
</pre></div>
</div>
<p align="center">
  <img src="../../../images/cute/tv_layout.png" alt="tv_layout.png" height="300"/>
</p>
<p>The above image is a visual representation of the above code. An arbitrary 4x8 layout of data is composed with a specific 8x4 TV-layout that represents a partitioning pattern. The result of the composition is on the right where each threads’ values are arranged across each row. The bottom layout depicts the inverse TV layout which shows the mapping of 4x8 logical coordinates to the thread id and value id they will be mapped to.</p>
<p>To see how these partitioning patterns are constructed and used, see the <a class="reference internal" href="0t_mma_atom.html"><span class="std std-doc">tutorial on building MMA Traits</span></a>.</p>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h2>
<section id="copy-a-subtile-from-global-memory-to-registers">
<h3>Copy a subtile from global memory to registers<a class="headerlink" href="#copy-a-subtile-from-global-memory-to-registers" title="Link to this heading">#</a></h3>
<p>The following example copies rows of a matrix (with any <code class="docutils literal notranslate"><span class="pre">Layout</span></code>)
from global memory to register memory,
then executes some algorithm <code class="docutils literal notranslate"><span class="pre">do_something</span></code>
on the row that lives in register memory.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">gmem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">{},</span><span class="w"> </span><span class="mi">16</span><span class="p">));</span><span class="w">  </span><span class="c1">// (_8,16)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor_like</span><span class="p">(</span><span class="n">gmem</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span><span class="w">                </span><span class="c1">// (_8)</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gmem</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">copy</span><span class="p">(</span><span class="n">gmem</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">),</span><span class="w"> </span><span class="n">rmem</span><span class="p">);</span>
<span class="w">  </span><span class="n">do_something</span><span class="p">(</span><span class="n">rmem</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This code does not need to know anything about the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> of <code class="docutils literal notranslate"><span class="pre">gmem</span></code>
other than that it is rank-2 and that the first mode has a static size.
The following code checks both of those conditions at compile time.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">CUTE_STATIC_ASSERT_V</span><span class="p">(</span><span class="n">rank</span><span class="p">(</span><span class="n">gmem</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">Int</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">{});</span>
<span class="n">CUTE_STATIC_ASSERT_V</span><span class="p">(</span><span class="n">is_static</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">shape</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gmem</span><span class="p">))</span><span class="o">&gt;</span><span class="p">{});</span>
</pre></div>
</div>
<p>Extending this example using the tiling utilities detailed in <a class="reference internal" href="02_layout_algebra.html"><span class="std std-doc">the <code class="docutils literal notranslate"><span class="pre">Layout</span></code> algebra section</span></a>, we can copy an arbitrary subtile of a tensor using almost the same code as above.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="n">gmem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">make_shape</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">));</span><span class="w">         </span><span class="c1">// (24,16)</span>

<span class="k">auto</span><span class="w"> </span><span class="n">tiler</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="n">Shape</span><span class="o">&lt;</span><span class="n">_8</span><span class="p">,</span><span class="n">_4</span><span class="o">&gt;</span><span class="p">{};</span><span class="w">                        </span><span class="c1">// 8x4 tiler</span>
<span class="c1">//auto tiler       = Tile&lt;Layout&lt;_8,_3&gt;, Layout&lt;_4,_2&gt;&gt;{};  // 8x4 tiler with stride-3 and stride-2</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">gmem_tiled</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">zipped_divide</span><span class="p">(</span><span class="n">gmem</span><span class="p">,</span><span class="w"> </span><span class="n">tiler</span><span class="p">);</span><span class="w">            </span><span class="c1">// ((_8,_4),Rest)</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">rmem</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">make_tensor_like</span><span class="p">(</span><span class="n">gmem_tiled</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">));</span><span class="w">    </span><span class="c1">// ((_8,_4))</span>
<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">gmem_tiled</span><span class="p">);</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">copy</span><span class="p">(</span><span class="n">gmem_tiled</span><span class="p">(</span><span class="n">_</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">),</span><span class="w"> </span><span class="n">rmem</span><span class="p">);</span>
<span class="w">  </span><span class="n">do_something</span><span class="p">(</span><span class="n">rmem</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This applies a statically shaped <code class="docutils literal notranslate"><span class="pre">Tiler</span></code> to the global memory <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, creates an register <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> that is compatible with the shape of that tile, then loops through each tile to copy it into memory and <code class="docutils literal notranslate"><span class="pre">do_something</span></code>.</p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Tensor</span></code> is defined as an <code class="docutils literal notranslate"><span class="pre">Engine</span></code> and a <code class="docutils literal notranslate"><span class="pre">Layout</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Engine</span></code> is an iterator that can be offset and dereferenced.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Layout</span></code> defines the logical domain of the tensor and maps coordinates to offsets.</p></li>
</ul>
</li>
<li><p>Tile a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> using the same methods for tiling <code class="docutils literal notranslate"><span class="pre">Layout</span></code>s.</p></li>
<li><p>Slice a <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> to retrieve subtensors.</p></li>
<li><p>Partitioning is tiling and/or composition followed by slicing.</p></li>
</ul>
</section>
<section id="copyright">
<h2>Copyright<a class="headerlink" href="#copyright" title="Link to this heading">#</a></h2>
<p>Copyright (c) 2017 - 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.
SPDX-License-Identifier: BSD-3-Clause</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">Redistribution</span> <span class="ow">and</span> <span class="n">use</span> <span class="ow">in</span> <span class="n">source</span> <span class="ow">and</span> <span class="n">binary</span> <span class="n">forms</span><span class="p">,</span> <span class="k">with</span> <span class="ow">or</span> <span class="n">without</span>
  <span class="n">modification</span><span class="p">,</span> <span class="n">are</span> <span class="n">permitted</span> <span class="n">provided</span> <span class="n">that</span> <span class="n">the</span> <span class="n">following</span> <span class="n">conditions</span> <span class="n">are</span> <span class="n">met</span><span class="p">:</span>

  <span class="mf">1.</span> <span class="n">Redistributions</span> <span class="n">of</span> <span class="n">source</span> <span class="n">code</span> <span class="n">must</span> <span class="n">retain</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span> <span class="n">this</span>
  <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span><span class="o">.</span>

  <span class="mf">2.</span> <span class="n">Redistributions</span> <span class="ow">in</span> <span class="n">binary</span> <span class="n">form</span> <span class="n">must</span> <span class="n">reproduce</span> <span class="n">the</span> <span class="n">above</span> <span class="n">copyright</span> <span class="n">notice</span><span class="p">,</span>
  <span class="n">this</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">conditions</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">following</span> <span class="n">disclaimer</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">documentation</span>
  <span class="ow">and</span><span class="o">/</span><span class="ow">or</span> <span class="n">other</span> <span class="n">materials</span> <span class="n">provided</span> <span class="k">with</span> <span class="n">the</span> <span class="n">distribution</span><span class="o">.</span>

  <span class="mf">3.</span> <span class="n">Neither</span> <span class="n">the</span> <span class="n">name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">copyright</span> <span class="n">holder</span> <span class="n">nor</span> <span class="n">the</span> <span class="n">names</span> <span class="n">of</span> <span class="n">its</span>
  <span class="n">contributors</span> <span class="n">may</span> <span class="n">be</span> <span class="n">used</span> <span class="n">to</span> <span class="n">endorse</span> <span class="ow">or</span> <span class="n">promote</span> <span class="n">products</span> <span class="n">derived</span> <span class="kn">from</span>
<span class="w">  </span><span class="nn">this</span> <span class="n">software</span> <span class="n">without</span> <span class="n">specific</span> <span class="n">prior</span> <span class="n">written</span> <span class="n">permission</span><span class="o">.</span>

  <span class="n">THIS</span> <span class="n">SOFTWARE</span> <span class="n">IS</span> <span class="n">PROVIDED</span> <span class="n">BY</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDERS</span> <span class="n">AND</span> <span class="n">CONTRIBUTORS</span> <span class="s2">&quot;AS IS&quot;</span>
  <span class="n">AND</span> <span class="n">ANY</span> <span class="n">EXPRESS</span> <span class="n">OR</span> <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span><span class="p">,</span> <span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">THE</span>
  <span class="n">IMPLIED</span> <span class="n">WARRANTIES</span> <span class="n">OF</span> <span class="n">MERCHANTABILITY</span> <span class="n">AND</span> <span class="n">FITNESS</span> <span class="n">FOR</span> <span class="n">A</span> <span class="n">PARTICULAR</span> <span class="n">PURPOSE</span> <span class="n">ARE</span>
  <span class="n">DISCLAIMED</span><span class="o">.</span> <span class="n">IN</span> <span class="n">NO</span> <span class="n">EVENT</span> <span class="n">SHALL</span> <span class="n">THE</span> <span class="n">COPYRIGHT</span> <span class="n">HOLDER</span> <span class="n">OR</span> <span class="n">CONTRIBUTORS</span> <span class="n">BE</span> <span class="n">LIABLE</span>
  <span class="n">FOR</span> <span class="n">ANY</span> <span class="n">DIRECT</span><span class="p">,</span> <span class="n">INDIRECT</span><span class="p">,</span> <span class="n">INCIDENTAL</span><span class="p">,</span> <span class="n">SPECIAL</span><span class="p">,</span> <span class="n">EXEMPLARY</span><span class="p">,</span> <span class="n">OR</span> <span class="n">CONSEQUENTIAL</span>
  <span class="n">DAMAGES</span> <span class="p">(</span><span class="n">INCLUDING</span><span class="p">,</span> <span class="n">BUT</span> <span class="n">NOT</span> <span class="n">LIMITED</span> <span class="n">TO</span><span class="p">,</span> <span class="n">PROCUREMENT</span> <span class="n">OF</span> <span class="n">SUBSTITUTE</span> <span class="n">GOODS</span> <span class="n">OR</span>
  <span class="n">SERVICES</span><span class="p">;</span> <span class="n">LOSS</span> <span class="n">OF</span> <span class="n">USE</span><span class="p">,</span> <span class="n">DATA</span><span class="p">,</span> <span class="n">OR</span> <span class="n">PROFITS</span><span class="p">;</span> <span class="n">OR</span> <span class="n">BUSINESS</span> <span class="n">INTERRUPTION</span><span class="p">)</span> <span class="n">HOWEVER</span>
  <span class="n">CAUSED</span> <span class="n">AND</span> <span class="n">ON</span> <span class="n">ANY</span> <span class="n">THEORY</span> <span class="n">OF</span> <span class="n">LIABILITY</span><span class="p">,</span> <span class="n">WHETHER</span> <span class="n">IN</span> <span class="n">CONTRACT</span><span class="p">,</span> <span class="n">STRICT</span> <span class="n">LIABILITY</span><span class="p">,</span>
  <span class="n">OR</span> <span class="n">TORT</span> <span class="p">(</span><span class="n">INCLUDING</span> <span class="n">NEGLIGENCE</span> <span class="n">OR</span> <span class="n">OTHERWISE</span><span class="p">)</span> <span class="n">ARISING</span> <span class="n">IN</span> <span class="n">ANY</span> <span class="n">WAY</span> <span class="n">OUT</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">USE</span>
  <span class="n">OF</span> <span class="n">THIS</span> <span class="n">SOFTWARE</span><span class="p">,</span> <span class="n">EVEN</span> <span class="n">IF</span> <span class="n">ADVISED</span> <span class="n">OF</span> <span class="n">THE</span> <span class="n">POSSIBILITY</span> <span class="n">OF</span> <span class="n">SUCH</span> <span class="n">DAMAGE</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./cute"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="02_layout_algebra.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">CuTe Layout Algebra</p>
      </div>
    </a>
    <a class="right-next"
       href="04_algorithms.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">CuTe Tensor algorithms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-operations">Fundamental operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-engines">Tensor Engines</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tagged-iterators">Tagged Iterators</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-creation">Tensor Creation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nonowning-tensors">Nonowning Tensors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#owning-tensors">Owning Tensors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accessing-a-tensor">Accessing a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tiling-a-tensor">Tiling a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#slicing-a-tensor">Slicing a Tensor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partitioning-a-tensor">Partitioning a Tensor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inner-and-outer-partitioning">Inner and outer partitioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thread-value-partitioning">Thread-Value partitioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#copy-a-subtile-from-global-memory-to-registers">Copy a subtile from global memory to registers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#copyright">Copyright</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>